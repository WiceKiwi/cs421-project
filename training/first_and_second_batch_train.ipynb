{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M196SD4NSQ-f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "from scipy.stats import entropy\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "data=np.load(\"../datasets/labeled/first_batch_multi_labels.npz\")\n",
        "data2=np.load(\"../datasets/labeled/second_batch_multi_labels.npz\")\n",
        "\n",
        "X1=data[\"X\"]\n",
        "y1=data[\"y\"]\n",
        "\n",
        "X2=data2[\"X\"]\n",
        "y2=data2[\"y\"]\n",
        "\n",
        "X=np.concatenate((X1,X2),axis=0)\n",
        "y=np.concatenate((y1,y2),axis=0)\n",
        "\n",
        "\n",
        "# X=data[\"X\"]\n",
        "# y=data[\"y\"]\n",
        "XX=pd.DataFrame(X)\n",
        "yy=pd.DataFrame(y)\n",
        "XX.rename(columns={0:\"user\",1:\"item\",2:\"rating\"},inplace=True)\n",
        "yy.rename(columns={0:\"user\",1:\"label\"},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF9v8FjBSS_8",
        "outputId": "2ce2a1f8-eb6e-4685-cf56-25bd3823acac"
      },
      "outputs": [],
      "source": [
        "# Merge labels into main dataset\n",
        "XX = XX.merge(yy, on=\"user\", how=\"left\")\n",
        "\n",
        "print(XX.shape)\n",
        "XX.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsN5fp2JSWY9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import skew, kurtosis, entropy\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "EPS = 1e-6\n",
        "\n",
        "def engineer_features(XX, yy):\n",
        "    user_features = XX.groupby(\"user\").agg(\n",
        "        review_count=(\"rating\", \"count\"),\n",
        "        avg_rating=(\"rating\", \"mean\"),\n",
        "        std_rating=(\"rating\", \"std\"),\n",
        "        like_count=(\"rating\", lambda x: (x == 10).sum()),\n",
        "        dislike_count=(\"rating\", lambda x: (x == -10).sum()),\n",
        "        unknown_count=(\"rating\", lambda x: (x == 1).sum()),\n",
        "        neutral_count=(\"rating\", lambda x: (x == 0).sum()),\n",
        "    ).reset_index()\n",
        "    \n",
        "    # --- PROPORTIONAL FEATURES ---\n",
        "    user_features[\"like_pct\"] = user_features[\"like_count\"] / (user_features[\"review_count\"] + EPS)\n",
        "    user_features[\"dislike_pct\"] = user_features[\"dislike_count\"] / (user_features[\"review_count\"] + EPS)\n",
        "    user_features[\"unknown_pct\"] = user_features[\"unknown_count\"] / (user_features[\"review_count\"] + EPS)\n",
        "    user_features[\"neutral_pct\"] = user_features[\"neutral_count\"] / (user_features[\"review_count\"] + EPS)\n",
        "    \n",
        "    # --- RATING DISTRIBUTION ENTROPY ---\n",
        "    def calc_entropy(row):\n",
        "        probs = [\n",
        "            row[\"like_pct\"], \n",
        "            row[\"dislike_pct\"], \n",
        "            row[\"unknown_pct\"], \n",
        "            row[\"neutral_pct\"]\n",
        "        ]\n",
        "        probs = [p for p in probs if p > 0]\n",
        "        return entropy(probs) if probs else 0\n",
        "    \n",
        "    user_features[\"rating_entropy\"] = user_features.apply(calc_entropy, axis=1)\n",
        "    \n",
        "    # --- MOVIE POPULARITY FEATURES ---\n",
        "    movie_popularity = XX.groupby(\"item\").size().reset_index(name=\"movie_popularity\")\n",
        "    XX_with_pop = XX.merge(movie_popularity, on=\"item\")\n",
        "    \n",
        "    pop_features = XX_with_pop.groupby(\"user\").agg(\n",
        "        avg_movie_popularity=(\"movie_popularity\", \"mean\"),\n",
        "        std_movie_popularity=(\"movie_popularity\", \"std\"),\n",
        "        min_movie_popularity=(\"movie_popularity\", \"min\"),\n",
        "        max_movie_popularity=(\"movie_popularity\", \"max\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    # Define movie popularity percentiles\n",
        "    movie_popularity = XX['item'].value_counts().reset_index()\n",
        "    movie_popularity.columns = ['item', 'popularity']\n",
        "    threshold = movie_popularity['popularity'].quantile(0.1)\n",
        "    rare_movies = movie_popularity[movie_popularity['popularity'] <= threshold]['item'].values\n",
        "    \n",
        "    XX['is_rare_movie'] = XX['item'].isin(rare_movies).astype(int)\n",
        "    \n",
        "    rare_stats = XX.groupby('user')['is_rare_movie'].mean().reset_index(name='rare_movies_watched_pct')\n",
        "    user_features = user_features.merge(rare_stats, on='user', how='left')\n",
        "    \n",
        "    # --- UNIQUE MOVIES AND DIVERSITY ---\n",
        "    unique_items = XX.groupby(\"user\")[\"item\"].nunique().reset_index()\n",
        "    unique_items.columns = [\"user\", \"unique_movies\"]\n",
        "    \n",
        "    # --- DEVIATION FROM POPULATION FEATURES ---\n",
        "    movie_avg_rating = XX.groupby(\"item\")[\"rating\"].mean().reset_index(name=\"movie_avg_rating\")\n",
        "    \n",
        "    XX_with_avg = XX.merge(movie_avg_rating, on=\"item\")\n",
        "    XX_with_avg[\"rating_deviation\"] = XX_with_avg[\"rating\"] - XX_with_avg[\"movie_avg_rating\"]\n",
        "    XX_with_avg[\"abs_rating_deviation\"] = np.abs(XX_with_avg[\"rating_deviation\"])\n",
        "    \n",
        "    deviation_features = XX_with_avg.groupby(\"user\").agg(\n",
        "        mean_deviation=(\"rating_deviation\", \"mean\"),\n",
        "        std_deviation=(\"rating_deviation\", \"std\"),\n",
        "        mean_abs_deviation=(\"abs_rating_deviation\", \"mean\"),\n",
        "        max_abs_deviation=(\"abs_rating_deviation\", \"max\"),\n",
        "    ).reset_index()\n",
        "    \n",
        "    # --- SEQUENTIAL PATTERN FEATURES ---\n",
        "    XX_sorted = XX.sort_values([\"user\", \"item\"])\n",
        "    XX_sorted[\"next_rating\"] = XX_sorted.groupby(\"user\")[\"rating\"].shift(-1)\n",
        "    XX_sorted[\"rating_diff\"] = XX_sorted[\"next_rating\"] - XX_sorted[\"rating\"]\n",
        "    XX_sorted[\"abs_rating_diff\"] = np.abs(XX_sorted[\"rating_diff\"])\n",
        "    \n",
        "    XX_sorted = XX_sorted.dropna(subset=[\"rating_diff\"])\n",
        "\n",
        "    XX_sorted[\"rating_direction\"] = XX_sorted[\"rating_diff\"].apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
        "    XX_sorted['direction_switch'] = XX_sorted['rating_direction'] != XX_sorted.groupby(\"user\")['rating_direction'].shift(1)\n",
        "    switch_count = XX_sorted.groupby(\"user\")[\"direction_switch\"].sum().reset_index(name=\"change_direction_count\")\n",
        "    user_features = user_features.merge(switch_count, on=\"user\", how=\"left\")\n",
        "    \n",
        "    sequence_features = XX_sorted.groupby(\"user\").agg(\n",
        "        mean_rating_diff=(\"rating_diff\", \"mean\"),\n",
        "        std_rating_diff=(\"rating_diff\", \"std\"),\n",
        "        mean_abs_rating_diff=(\"abs_rating_diff\", \"mean\"),\n",
        "        max_abs_rating_diff=(\"abs_rating_diff\", \"max\"),\n",
        "        rating_changes_count=(\"rating_diff\", lambda x: (x != 0).sum()),\n",
        "    ).reset_index()\n",
        "    \n",
        "    sequence_features[\"rating_changes_pct\"] = sequence_features[\"rating_changes_count\"] / (\n",
        "        user_features.set_index(\"user\")[\"review_count\"] - 1 + EPS\n",
        "    ).reindex(sequence_features[\"user\"]).values\n",
        "    \n",
        "    \n",
        "    # --- COMBINE ALL USER-LEVEL FEATURES ---\n",
        "    all_features = user_features.merge(pop_features, on=\"user\", how=\"left\")\n",
        "    all_features = all_features.merge(unique_items, on=\"user\", how=\"left\")\n",
        "    all_features = all_features.merge(deviation_features, on=\"user\", how=\"left\")\n",
        "    all_features = all_features.merge(sequence_features, on=\"user\", how=\"left\")\n",
        "    all_features = all_features.fillna(0)\n",
        "    \n",
        "    # --- UNSUPERVISED ANOMALY DETECTION FEATURES ---\n",
        "    feature_cols = [col for col in all_features.columns \n",
        "                   if col not in [\"user\", \"label\", \"is_anomalous\"] \n",
        "                   and all_features[col].dtype in [np.float64, np.int64]]\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(all_features[feature_cols])\n",
        "    \n",
        "    # --- COMBINE WITH CLASS LABELS ---\n",
        "    all_features = all_features.merge(yy, on=\"user\", how=\"left\")\n",
        "    all_features[\"is_anomalous\"] = (all_features[\"label\"] != 0).astype(int)\n",
        "    \n",
        "    # --- ADVANCED INTERACTION FEATURES ---\n",
        "    all_features[\"like_dislike_ratio\"] = all_features[\"like_count\"] / (all_features[\"dislike_count\"] + EPS)\n",
        "    all_features[\"rating_range\"] = all_features[\"max_abs_rating_diff\"]\n",
        "    all_features[\"popularity_vs_deviation\"] = all_features[\"avg_movie_popularity\"] * all_features[\"mean_abs_deviation\"]\n",
        "    all_features[\"entropy_by_count\"] = all_features[\"rating_entropy\"] * np.log1p(all_features[\"review_count\"])\n",
        "    \n",
        "    # --- BINNING FEATURES ---\n",
        "    all_features[\"review_count_bin\"] = pd.qcut(all_features[\"review_count\"], \n",
        "                                             q=5, labels=False, duplicates=\"drop\")\n",
        "    \n",
        "    return all_features\n",
        "\n",
        "\n",
        "user_features = engineer_features(XX, yy)\n",
        "\n",
        "min_max_df = XX.groupby(\"user\")[\"item\"].agg(min_movie=\"min\", max_movie=\"max\", median_movie=\"median\", variance_movie=\"var\").reset_index()\n",
        "user_features = user_features.merge(min_max_df, on=\"user\", how=\"left\")\n",
        "\n",
        "XX[\"item_rating\"] = XX[\"item\"] * XX[\"rating\"]\n",
        "sum_rating = XX.groupby(\"user\")[\"rating\"].sum().reset_index(name=\"sum_rating\")\n",
        "sum_product = XX.groupby(\"user\")[\"item_rating\"].sum().reset_index(name=\"sum_item_rating\")\n",
        "\n",
        "user_features = user_features.merge(sum_product, on=\"user\", how=\"left\")\n",
        "user_features = user_features.merge(sum_rating, on=\"user\", how=\"left\")\n",
        "\n",
        "user_features[\"average_product\"] = user_features[\"sum_item_rating\"] / (user_features[\"review_count\"])\n",
        "\n",
        "user_features[\"product_above_zero\"] = (user_features[\"sum_item_rating\"] > 0).astype(int)\n",
        "user_features[\"sum_above_zero\"] = (user_features[\"sum_rating\"] > 0).astype(int)\n",
        "user_features[\"avg_product_vs_avg_rating\"] = user_features[\"average_product\"] / (user_features[\"avg_rating\"] + EPS)\n",
        "\n",
        "# --- Round ratio columns to 2 decimals ---\n",
        "ratio_cols = [col for col in user_features.columns if col.endswith(\"_ratio\")]\n",
        "for c in ratio_cols:\n",
        "    user_features[c] = user_features[c].round(2)\n",
        "\n",
        "XX_sorted = XX.sort_values(by=['user', 'item'])\n",
        "XX_sorted['item_diff'] = XX_sorted.groupby('user')['item'].diff().fillna(0)\n",
        "gap_stats = XX_sorted.groupby('user')['item_diff'].agg(['mean', 'std', 'max'])\n",
        "gap_stats.columns = ['gap_mean', 'gap_std', 'gap_max']\n",
        "user_features = user_features.merge(gap_stats, on='user', how='left')\n",
        "\n",
        "# Final preview\n",
        "print(user_features.shape)\n",
        "# user_features.to_csv(\"user_features.csv\", index=False)\n",
        "user_features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "7VKt4RkVSYIh",
        "outputId": "fef9626a-74d2-484a-cedb-a2c2fc60813b"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "df = user_features.copy()\n",
        "df = df.drop(columns=\"is_anomalous\")\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "dIjaIB91SbZy",
        "outputId": "baab7aeb-0742-48ba-d83c-7c3cb8bbb4f6"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=[\"user\"], inplace=True, errors=\"ignore\")\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", annot=False, fmt=\".2f\")\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TmQ09yYScyW",
        "outputId": "d5589bcf-a3d0-4814-c939-1b92b7d359c7"
      },
      "outputs": [],
      "source": [
        "correlated_features = set()\n",
        "threshold = 0.90\n",
        "\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            correlated_features.add(colname)\n",
        "\n",
        "df.drop(columns=correlated_features, inplace=True, errors=\"ignore\")\n",
        "print(f\"Dropped correlated features: {correlated_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "G1HMkMJiSfBZ",
        "outputId": "f05906c2-a272-4e8d-c39e-7a6f049a54a3"
      },
      "outputs": [],
      "source": [
        "correlation_matrix = df.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", annot=False, fmt=\".2f\")\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "vEBCy3V3SgrV",
        "outputId": "c1b8bd5b-3149-4d69-ecb2-0fbc634d4fc6"
      },
      "outputs": [],
      "source": [
        "# Define features and target\n",
        "X = df.drop(columns=[\"label\"])\n",
        "y = df[\"label\"]\n",
        "\n",
        "print(X.columns)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPTpSwrMSkF_",
        "outputId": "17c78c6a-6e05-413d-edce-5dcbba40df2f"
      },
      "outputs": [],
      "source": [
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "print(\"🔹 Original Class Distribution:\", Counter(y))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=16)\n",
        "\n",
        "smote_tomek = SMOTETomek(random_state=16)\n",
        "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"🔹 Resampled Class Distribution (After SMOTE):\", Counter(y_train_resampled))\n",
        "\n",
        "poly = PolynomialFeatures(degree=1, interaction_only=False, include_bias=True)\n",
        "scaler = StandardScaler()\n",
        "X_train_poly = poly.fit_transform(scaler.fit_transform(X_train_resampled))\n",
        "X_test_poly = poly.transform(scaler.transform(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import optuna\n",
        "# from sklearn.metrics import roc_auc_score\n",
        "# import xgboost as xgb\n",
        "\n",
        "# def objective(trial):\n",
        "#     params = {\n",
        "#         \"objective\": \"multi:softprob\",\n",
        "#         \"num_class\": len(np.unique(y_train_resampled)),\n",
        "#         \"eval_metric\": \"mlogloss\",\n",
        "#         \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
        "#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "#         \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9),\n",
        "#         \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 5),\n",
        "#         \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
        "#         \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
        "#         \"gamma\": trial.suggest_uniform(\"gamma\", 0, 0.2),\n",
        "#         \"random_state\": 16,\n",
        "#         \"n_jobs\": -1\n",
        "#     }\n",
        "\n",
        "#     model = xgb.XGBClassifier(**params)\n",
        "#     model.fit(X_train_poly, y_train_resampled)\n",
        "#     y_pred_proba = model.predict_proba(X_test_poly)\n",
        "    \n",
        "#     auc_scores = []\n",
        "#     for i in range(y_pred_proba.shape[1]):\n",
        "#         binary_true = (y_test == i).astype(int)\n",
        "#         try:\n",
        "#             auc = roc_auc_score(binary_true, y_pred_proba[:, i])\n",
        "#             auc_scores.append(auc)\n",
        "#         except:\n",
        "#             auc_scores.append(0)\n",
        "    \n",
        "#     return np.mean(auc_scores)\n",
        "\n",
        "# study = optuna.create_study(direction=\"maximize\")\n",
        "# study.optimize(objective, n_trials=50)\n",
        "\n",
        "# best_params = study.best_params\n",
        "# print(\"Best Parameters:\", best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "# Initialize and train the XGBoost classifier for multi-class prediction\n",
        "num_classes = len(np.unique(y))\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    num_class=num_classes,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=16,\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.0674590634280927,\n",
        "    max_depth=9,\n",
        "    min_child_weight=2,\n",
        "    subsample=0.7807606824615063,\n",
        "    colsample_bytree=0.7006957833429297,\n",
        "    gamma=0.16240296667488197,\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_poly, y_train_resampled)\n",
        "\n",
        "y_pred_proba_xgb = xgb_model.predict_proba(X_test_poly)\n",
        "\n",
        "print(\"XGBoost AUC Scores per Class:\")\n",
        "auc_per_class_xgb = {}\n",
        "for i in range(y_pred_proba_xgb.shape[1]):\n",
        "    binary_true = (y_test == i).astype(int)\n",
        "    try:\n",
        "        auc = roc_auc_score(binary_true, y_pred_proba_xgb[:, i])\n",
        "        auc_per_class_xgb[i] = auc\n",
        "        print(f\"  Class {i}: AUC = {auc:.3f}\")\n",
        "    except Exception as e:\n",
        "        auc_per_class_xgb[i] = None\n",
        "        print(f\"  Class {i}: AUC could not be computed\")\n",
        "\n",
        "k = 5\n",
        "AUC_0 = auc_per_class_xgb[0]\n",
        "anomaly_aucs = [auc_per_class_xgb[i] for i in range(1, k+1) if i in auc_per_class_xgb]\n",
        "\n",
        "final_metric = (0.5 * AUC_0) + (0.5 / k) * sum(anomaly_aucs)\n",
        "\n",
        "print(f\"\\nFinal Evaluation Metric: {final_metric:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Save objects\n",
        "model_features = list(X_train_resampled.columns)\n",
        "joblib.dump(model_features, \"model_features.pkl\")\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "joblib.dump(poly, \"poly.pkl\")\n",
        "joblib.dump(xgb_model, \"model.pkl\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "CS421",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
