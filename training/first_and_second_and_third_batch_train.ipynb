{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M196SD4NSQ-f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "from scipy.stats import entropy\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append(os.path.abspath(\"..\"))\n",
        "\n",
        "\n",
        "SEED = 1\n",
        "\n",
        "data=np.load(\"../datasets/labeled/first_batch_multi_labels.npz\")\n",
        "data2=np.load(\"../datasets/labeled/second_batch_multi_labels.npz\")\n",
        "data3=np.load(\"../datasets/labeled/third_batch_multi_labels.npz\")\n",
        "\n",
        "X1=data[\"X\"]\n",
        "y1=data[\"y\"]\n",
        "\n",
        "X2=data2[\"X\"]\n",
        "y2=data2[\"y\"]\n",
        "\n",
        "X3=data3[\"X\"]\n",
        "y3=data3[\"y\"]\n",
        "\n",
        "X=np.concatenate((X1,X2,X3),axis=0)\n",
        "y=np.concatenate((y1,y2,y3),axis=0)\n",
        "\n",
        "XX=pd.DataFrame(X)\n",
        "yy=pd.DataFrame(y)\n",
        "XX.rename(columns={0:\"user\",1:\"item\",2:\"rating\"},inplace=True)\n",
        "yy.rename(columns={0:\"user\",1:\"label\"},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF9v8FjBSS_8",
        "outputId": "2ce2a1f8-eb6e-4685-cf56-25bd3823acac"
      },
      "outputs": [],
      "source": [
        "# Merge labels into main dataset\n",
        "XX = XX.merge(yy, on=\"user\", how=\"left\").sort_values(by=[\"user\", \"item\"]).reset_index(drop=True)\n",
        "\n",
        "print(XX.shape)\n",
        "XX.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsN5fp2JSWY9"
      },
      "outputs": [],
      "source": [
        "from utils.feature_transformation import aggregate_features\n",
        "\n",
        "user_features = aggregate_features(XX)\n",
        "user_features = user_features.merge(yy, on=\"user\", how=\"left\")\n",
        "print(user_features.shape)\n",
        "# user_features.to_csv(\"user_features.csv\", index=False)\n",
        "user_features.head()\n",
        "\n",
        "# Your selected features\n",
        "selected_columns = [ 'user', 'label',\n",
        "    'avg_rating', 'contrarian_lowly_rated_like_pct', 'dislike_pct',\n",
        "    'dislikes_popular', 'dominant_rating', 'gap_max', 'gap_mean', 'likes_rare',\n",
        "    'max_abs_deviation', 'max_movie', 'max_movie_popularity',\n",
        "    'mean_abs_deviation', 'mean_item_rating_median', 'mean_rating_diff',\n",
        "    'median_movie', 'min_movie', 'min_movie_popularity', 'neutral_pct',\n",
        "    'neutral_rare', 'popularity_vs_deviation', 'rare_movies_watched_pct',\n",
        "    'rating_changes_pct', 'std_deviation', 'std_movie_popularity', 'std_rating',\n",
        "    'sum_item_rating', 'unknown_popular', 'unknown_rare',\n",
        "    'user_pop_percentile_std', 'variance_movie', 'z_rating_max',\n",
        "    'z_rating_median'\n",
        "]\n",
        "\n",
        "# Add target column back (if you want to keep it)\n",
        "if 'target' in user_features.columns:\n",
        "    selected_columns.append('target')\n",
        "\n",
        "# Filter the dataframe\n",
        "user_features = user_features[[col for col in selected_columns if col in user_features.columns]]\n",
        "\n",
        "# Confirm shape/output\n",
        "print(user_features.shape)\n",
        "user_features.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "7VKt4RkVSYIh",
        "outputId": "fef9626a-74d2-484a-cedb-a2c2fc60813b"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "df = user_features.copy()\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "dIjaIB91SbZy",
        "outputId": "baab7aeb-0742-48ba-d83c-7c3cb8bbb4f6"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=[\"user\"], inplace=True)\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", annot=False, fmt=\".2f\")\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TmQ09yYScyW",
        "outputId": "d5589bcf-a3d0-4814-c939-1b92b7d359c7"
      },
      "outputs": [],
      "source": [
        "correlated_features = set()\n",
        "threshold = 0.9\n",
        "\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            correlated_features.add(colname)\n",
        "\n",
        "df.drop(columns=correlated_features, inplace=True, errors=\"ignore\")\n",
        "print(f\"Dropped correlated features: {correlated_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "G1HMkMJiSfBZ",
        "outputId": "f05906c2-a272-4e8d-c39e-7a6f049a54a3"
      },
      "outputs": [],
      "source": [
        "correlation_matrix = df.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", annot=False, fmt=\".2f\")\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "vEBCy3V3SgrV",
        "outputId": "c1b8bd5b-3149-4d69-ecb2-0fbc634d4fc6"
      },
      "outputs": [],
      "source": [
        "# Define features and target\n",
        "X = df.drop(columns=[\"label\"])\n",
        "y = df[\"label\"]\n",
        "\n",
        "print(X.columns)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPTpSwrMSkF_",
        "outputId": "17c78c6a-6e05-413d-edce-5dcbba40df2f"
      },
      "outputs": [],
      "source": [
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "print(\"ðŸ”¹ Original Class Distribution:\", Counter(y))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=SEED)\n",
        "\n",
        "smote = SMOTE(sampling_strategy='not majority', random_state=SEED)\n",
        "X_train_over, y_train_over = smote.fit_resample(X_train, y_train)\n",
        "print(\"ðŸ”¹ Distribution after SMOTE (only minority oversampled):\", Counter(y_train_over))\n",
        "\n",
        "tomek = TomekLinks(sampling_strategy='not majority')\n",
        "X_train_resampled, y_train_resampled = tomek.fit_resample(X_train_over, y_train_over)\n",
        "print(\"ðŸ”¹ Final Resampled Distribution (After applying Tomek on minority):\", Counter(y_train_resampled))\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "import json\n",
        "\n",
        "params_file = \"best_xgb_params.json\"\n",
        "\n",
        "if os.path.exists(params_file):\n",
        "    print(\"Hyperparameters found.\")\n",
        "    with open(params_file, \"r\") as f:\n",
        "        best_params = json.load(f)\n",
        "else:\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"objective\": \"multi:softprob\",\n",
        "            \"num_class\": len(np.unique(y_train_resampled)),\n",
        "            \"eval_metric\": \"mlogloss\",\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9),\n",
        "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 5),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "            \"gamma\": trial.suggest_float(\"gamma\", 0, 0.2),\n",
        "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True),\n",
        "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True),\n",
        "            \"random_state\": SEED,\n",
        "            \"tree_method\": \"hist\",\n",
        "            \"predictor\": \"cpu_predictor\",\n",
        "            \"verbosity\": 0,\n",
        "            \"n_jobs\": 1\n",
        "        }\n",
        "\n",
        "        model = xgb.XGBClassifier(**params)\n",
        "        model.fit(X_train_scaled, y_train_resampled)\n",
        "        y_pred_proba = model.predict_proba(X_test_scaled)\n",
        "\n",
        "        auc_scores = []\n",
        "        for i in range(y_pred_proba.shape[1]):\n",
        "            binary_true = (y_test == i).astype(int)\n",
        "            try:\n",
        "                auc = roc_auc_score(binary_true, y_pred_proba[:, i])\n",
        "                auc_scores.append(auc)\n",
        "            except:\n",
        "                auc_scores.append(0)\n",
        "\n",
        "        return np.mean(auc_scores)\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=80)\n",
        "\n",
        "    best_params = study.best_params\n",
        "\n",
        "    with open(params_file, \"w\") as f:\n",
        "        json.dump(best_params, f)\n",
        "\n",
        "    print(best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# num_classes = len(np.unique(y))\n",
        "# xgb_model = xgb.XGBClassifier(\n",
        "#     **best_params,\n",
        "#     objective='multi:softprob',\n",
        "#     num_class=num_classes,\n",
        "#     eval_metric='mlogloss',\n",
        "#     random_state=SEED,\n",
        "#     reg_lambda=0.1,\n",
        "#     n_jobs=1,\n",
        "# )\n",
        "\n",
        "# xgb_model.fit(X_train_scaled, y_train_resampled)\n",
        "\n",
        "# y_pred_proba_xgb = xgb_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# print(\"XGBoost AUC Scores per Class:\")\n",
        "# auc_per_class_xgb = {}\n",
        "# for i in range(y_pred_proba_xgb.shape[1]):\n",
        "#     binary_true = (y_test == i).astype(int)\n",
        "#     try:\n",
        "#         auc = roc_auc_score(binary_true, y_pred_proba_xgb[:, i])\n",
        "#         auc_per_class_xgb[i] = auc\n",
        "#         print(f\"  Class {i}: AUC = {auc:.3f}\")\n",
        "#     except Exception as e:\n",
        "#         auc_per_class_xgb[i] = None\n",
        "#         print(f\"  Class {i}: AUC could not be computed\")\n",
        "\n",
        "# k = 5\n",
        "# AUC_0 = auc_per_class_xgb[0]\n",
        "# anomaly_aucs = [auc_per_class_xgb[i] for i in range(1, k+1) if i in auc_per_class_xgb]\n",
        "\n",
        "# final_metric = (0.5 * AUC_0) + (0.5 / k) * sum(anomaly_aucs)\n",
        "\n",
        "# print(f\"\\nFinal Evaluation Metric: {final_metric:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import xgboost as xgb\n",
        "\n",
        "model_features = list(X_train.columns)\n",
        "\n",
        "# Combine train and test\n",
        "X_full = np.concatenate((X_train, X_test), axis=0)\n",
        "y_full = np.concatenate((y_train, y_test), axis=0)\n",
        "print(\"ðŸ”¹ Original Class Distribution (Full Dataset):\", Counter(y_full))\n",
        "\n",
        "# Setting sampling_strategy='not majority' means only classes other than the majority are oversampled.\n",
        "smote = SMOTE(sampling_strategy='not majority', random_state=SEED)\n",
        "X_over, y_over = smote.fit_resample(X_full, y_full)\n",
        "print(\"ðŸ”¹ Distribution after SMOTE (only minority oversampled):\", Counter(y_over))\n",
        "\n",
        "tomek = TomekLinks(sampling_strategy='not majority')\n",
        "X_resampled, y_resampled = tomek.fit_resample(X_over, y_over)\n",
        "print(\"ðŸ”¹ Final Resampled Distribution (After applying Tomek on minority):\", Counter(y_resampled))\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(pd.DataFrame(X_resampled, columns=model_features))\n",
        "\n",
        "num_classes = len(np.unique(y_full))\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    **best_params,\n",
        "    objective='multi:softprob',\n",
        "    num_class=num_classes,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=SEED,\n",
        "    reg_lambda=0.1,\n",
        "    n_jobs=1,\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_scaled, y_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Save objects\n",
        "joblib.dump(model_features, \"../testing/model_features.pkl\")\n",
        "joblib.dump(scaler, \"../testing/scaler.pkl\")\n",
        "joblib.dump(xgb_model, \"../testing/xgb_model.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "explainer = shap.TreeExplainer(xgb_model, X_train_scaled)\n",
        "shap_values = explainer(X_test_scaled[:100], check_additivity=False)\n",
        "\n",
        "shap_array = shap_values.values  # shape: (n_samples, n_features, n_classes)\n",
        "\n",
        "# Loop over classes and show SHAP summary\n",
        "for class_id in range(num_classes):\n",
        "    print(f\"\\nðŸ“Š SHAP Summary for Class {class_id}\")\n",
        "    shap.summary_plot(\n",
        "        shap_array[:, :, class_id],  # SHAP values for this class\n",
        "        X_test_scaled[:100],\n",
        "        feature_names=model_features,\n",
        "        show=True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if isinstance(X_test_scaled, np.ndarray):\n",
        "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=model_features)\n",
        "\n",
        "for class_id in range(num_classes):\n",
        "    print(f\"\\nðŸ§  SHAP Decision Plot for Class {class_id}\")\n",
        "    \n",
        "    shap.decision_plot(\n",
        "        explainer.expected_value[class_id],\n",
        "        shap_array[:10, :, class_id],\n",
        "        X_test_scaled.iloc[:10],\n",
        "        feature_names=model_features,\n",
        "        show=True\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "CS421",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
