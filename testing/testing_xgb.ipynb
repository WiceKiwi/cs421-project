{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change the np.load to include the file\n",
    "- Add the following .pkl files into the same folder as this .ipynb (model, model_features, scaler, poly)\n",
    "- Edit the transform_features() method if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZrJYQUWBRE8",
    "outputId": "0992887d-734b-47ee-e5f7-72badc38ad9a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "import joblib\n",
    "import networkx as nx\n",
    "\n",
    "test=np.load(\"../datasets/labeled/third_batch_multi_labels.npz\")\n",
    "\n",
    "X_test=test[\"X\"]\n",
    "\n",
    "XX_test = pd.DataFrame(X_test)\n",
    "XX_test.rename(columns={0:\"user\",1:\"item\",2:\"rating\"},inplace=True)\n",
    "\n",
    "num_unique_users = XX_test[\"user\"].nunique()\n",
    "print(f\"Number of unique user IDs in the test set: {num_unique_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hprjtSwFBMA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "def transform_features(X):\n",
    "    \"\"\"\n",
    "    Transforms the input DataFrame X (assumed to have columns 'user', 'item', 'rating')\n",
    "    into a DataFrame with aggregated user-level features. These include:\n",
    "      - Aggregation of counts and summary statistics.\n",
    "      - Proportional features.\n",
    "      - Gap statistics on item IDs.\n",
    "      - Min, max, median, and variance of movie IDs.\n",
    "      - Product features based on item and rating.\n",
    "      - Rating distribution entropy.\n",
    "      - Movie popularity features.\n",
    "      - Unique movies count and diversity.\n",
    "      - Deviation from population average rating.\n",
    "      - Sequential pattern features (rating differences, etc.).\n",
    "      - Advanced interaction features and review count binning.\n",
    "\n",
    "    Returns a DataFrame with all the computed features.\n",
    "    \"\"\"\n",
    "    df = X.copy()\n",
    "\n",
    "    # --- AGGREGATION: Compute counts and summary statistics ---\n",
    "    user_features = df.groupby(\"user\").agg(\n",
    "        review_count=(\"rating\", \"count\"),\n",
    "        avg_rating=(\"rating\", \"mean\"),\n",
    "        std_rating=(\"rating\", \"std\"),\n",
    "        like_count=(\"rating\", lambda x: (x == 10).sum()),\n",
    "        dislike_count=(\"rating\", lambda x: (x == -10).sum()),\n",
    "        unknown_count=(\"rating\", lambda x: (x == 1).sum()),\n",
    "        neutral_count=(\"rating\", lambda x: (x == 0).sum()),\n",
    "    ).reset_index()\n",
    "\n",
    "    # --- PROPORTIONAL FEATURES ---\n",
    "    user_features[\"like_pct\"] = user_features[\"like_count\"] / (user_features[\"review_count\"] + EPS)\n",
    "    user_features[\"dislike_pct\"] = user_features[\"dislike_count\"] / (user_features[\"review_count\"] + EPS)\n",
    "    user_features[\"unknown_pct\"] = user_features[\"unknown_count\"] / (user_features[\"review_count\"] + EPS)\n",
    "    user_features[\"neutral_pct\"] = user_features[\"neutral_count\"] / (user_features[\"review_count\"] + EPS)\n",
    "\n",
    "    # --- GAP STATISTICS ---\n",
    "    XX_sorted = X.sort_values(by=['user', 'item'])\n",
    "    XX_sorted['item_diff'] = XX_sorted.groupby('user')['item'].diff().fillna(0)\n",
    "    gap_stats = XX_sorted.groupby('user')['item_diff'].agg(['mean', 'std', 'max', 'min'])\n",
    "    gap_stats.columns = ['gap_mean', 'gap_std', 'gap_max', 'gap_min']\n",
    "    user_features = user_features.merge(gap_stats, on='user', how='left')\n",
    "\n",
    "    # --- MIN/MAX/MEDIAN/VARIANCE OF MOVIE IDs ---\n",
    "    min_max_df = X.groupby(\"user\")[\"item\"].agg(min_movie=\"min\", max_movie=\"max\", median_movie=\"median\", variance_movie=\"var\").reset_index()\n",
    "    user_features = user_features.merge(min_max_df, on=\"user\", how=\"left\")\n",
    "\n",
    "    # --- PRODUCT FEATURES ---\n",
    "    X = X.copy()  # work on a copy to avoid modifying original data\n",
    "    X[\"item_rating\"] = X[\"item\"] * X[\"rating\"]\n",
    "    sum_rating = X.groupby(\"user\")[\"rating\"].sum().reset_index(name=\"sum_rating\")\n",
    "    sum_product = X.groupby(\"user\")[\"item_rating\"].sum().reset_index(name=\"sum_item_rating\")\n",
    "    user_features = user_features.merge(sum_product, on=\"user\", how=\"left\")\n",
    "    user_features = user_features.merge(sum_rating, on=\"user\", how=\"left\")\n",
    "\n",
    "    user_features[\"average_product\"] = user_features[\"sum_item_rating\"] / user_features[\"review_count\"]\n",
    "    user_features[\"product_above_zero\"] = (user_features[\"sum_item_rating\"] > 0).astype(int)\n",
    "    user_features[\"sum_above_zero\"] = (user_features[\"sum_rating\"] > 0).astype(int)\n",
    "    user_features[\"avg_product_vs_avg_rating\"] = user_features[\"average_product\"] / (user_features[\"avg_rating\"] + EPS)\n",
    "\n",
    "    # --- RATING DISTRIBUTION ENTROPY ---\n",
    "    def calc_entropy(row):\n",
    "        probs = [row[\"like_pct\"], row[\"dislike_pct\"], row[\"unknown_pct\"], row[\"neutral_pct\"]]\n",
    "        probs = [p for p in probs if p > 0]\n",
    "        return entropy(probs) if probs else 0\n",
    "    user_features[\"rating_entropy\"] = user_features.apply(calc_entropy, axis=1)\n",
    "\n",
    "    # --- MOVIE POPULARITY FEATURES ---\n",
    "    movie_popularity = X.groupby(\"item\").size().reset_index(name=\"movie_popularity\")\n",
    "    X_with_pop = X.merge(movie_popularity, on=\"item\")\n",
    "    pop_features = X_with_pop.groupby(\"user\").agg(\n",
    "        avg_movie_popularity=(\"movie_popularity\", \"mean\"),\n",
    "        std_movie_popularity=(\"movie_popularity\", \"std\"),\n",
    "        min_movie_popularity=(\"movie_popularity\", \"min\"),\n",
    "        max_movie_popularity=(\"movie_popularity\", \"max\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Define movie popularity percentiles\n",
    "    movie_popularity = X['item'].value_counts().reset_index()\n",
    "    movie_popularity.columns = ['item', 'popularity']\n",
    "    threshold = movie_popularity['popularity'].quantile(0.1)\n",
    "    rare_movies = movie_popularity[movie_popularity['popularity'] <= threshold]['item'].values\n",
    "    \n",
    "    X['is_rare_movie'] = X['item'].isin(rare_movies).astype(int)\n",
    "    \n",
    "    rare_stats = X.groupby('user')['is_rare_movie'].mean().reset_index(name='rare_movies_watched_pct')\n",
    "    user_features = user_features.merge(rare_stats, on='user', how='left')\n",
    "\n",
    "    # --- UNIQUE MOVIES AND DIVERSITY ---\n",
    "    unique_items = X.groupby(\"user\")[\"item\"].nunique().reset_index()\n",
    "    unique_items.columns = [\"user\", \"unique_movies\"]\n",
    "\n",
    "    # --- DEVIATION FROM POPULATION FEATURES ---\n",
    "    movie_avg_rating = X.groupby(\"item\")[\"rating\"].mean().reset_index(name=\"movie_avg_rating\")\n",
    "    X_with_avg = X.merge(movie_avg_rating, on=\"item\")\n",
    "    X_with_avg[\"rating_deviation\"] = X_with_avg[\"rating\"] - X_with_avg[\"movie_avg_rating\"]\n",
    "    X_with_avg[\"abs_rating_deviation\"] = np.abs(X_with_avg[\"rating_deviation\"])\n",
    "    deviation_features = X_with_avg.groupby(\"user\").agg(\n",
    "        mean_deviation=(\"rating_deviation\", \"mean\"),\n",
    "        std_deviation=(\"rating_deviation\", \"std\"),\n",
    "        mean_abs_deviation=(\"abs_rating_deviation\", \"mean\"),\n",
    "        max_abs_deviation=(\"abs_rating_deviation\", \"max\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # --- SEQUENTIAL PATTERN FEATURES ---\n",
    "    X_sorted = X.sort_values([\"user\", \"item\"])\n",
    "    X_sorted[\"next_rating\"] = X_sorted.groupby(\"user\")[\"rating\"].shift(-1)\n",
    "    X_sorted[\"rating_diff\"] = X_sorted[\"next_rating\"] - X_sorted[\"rating\"]\n",
    "    X_sorted[\"abs_rating_diff\"] = np.abs(X_sorted[\"rating_diff\"])\n",
    "    X_sorted = X_sorted.dropna(subset=[\"rating_diff\"])\n",
    "    sequence_features = X_sorted.groupby(\"user\").agg(\n",
    "        mean_rating_diff=(\"rating_diff\", \"mean\"),\n",
    "        std_rating_diff=(\"rating_diff\", \"std\"),\n",
    "        mean_abs_rating_diff=(\"abs_rating_diff\", \"mean\"),\n",
    "        max_abs_rating_diff=(\"abs_rating_diff\", \"max\"),\n",
    "        rating_changes_count=(\"rating_diff\", lambda x: (x != 0).sum()),\n",
    "    ).reset_index()\n",
    "    sequence_features[\"rating_changes_pct\"] = sequence_features[\"rating_changes_count\"] / (\n",
    "        user_features.set_index(\"user\")[\"review_count\"] - 1 + EPS\n",
    "    ).reindex(sequence_features[\"user\"]).values\n",
    "\n",
    "    X_sorted[\"rating_direction\"] = X_sorted[\"rating_diff\"].apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    X_sorted['direction_switch'] = X_sorted['rating_direction'] != X_sorted.groupby(\"user\")['rating_direction'].shift(1)\n",
    "    switch_count = X_sorted.groupby(\"user\")[\"direction_switch\"].sum().reset_index(name=\"change_direction_count\")\n",
    "    user_features = user_features.merge(switch_count, on=\"user\", how=\"left\")\n",
    "\n",
    "    # --- COMBINE ALL FEATURES ---\n",
    "    all_features = user_features.merge(pop_features, on=\"user\", how=\"left\")\n",
    "    all_features = all_features.merge(unique_items, on=\"user\", how=\"left\")\n",
    "    all_features = all_features.merge(deviation_features, on=\"user\", how=\"left\")\n",
    "    all_features = all_features.merge(sequence_features, on=\"user\", how=\"left\")\n",
    "\n",
    "    all_features = all_features.fillna(0)\n",
    "\n",
    "    # --- ADVANCED INTERACTION FEATURES ---\n",
    "    all_features[\"like_dislike_ratio\"] = all_features[\"like_count\"] / (all_features[\"dislike_count\"] + EPS)\n",
    "    all_features[\"rating_range\"] = all_features[\"max_abs_rating_diff\"]\n",
    "    all_features[\"popularity_vs_deviation\"] = all_features[\"avg_movie_popularity\"] * all_features[\"mean_abs_deviation\"]\n",
    "    all_features[\"entropy_by_count\"] = all_features[\"rating_entropy\"] * np.log1p(all_features[\"review_count\"])\n",
    "\n",
    "    # --- BINNING FEATURES ---\n",
    "    all_features[\"review_count_bin\"] = pd.qcut(all_features[\"review_count\"], q=5, labels=False, duplicates=\"drop\")\n",
    "\n",
    "    return pd.DataFrame(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRt-PQQ4Biv6",
    "outputId": "6a93053f-949f-462c-ff0e-5bce3e2fa995"
   },
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "test_features = transform_features(XX_test)\n",
    "test_features \n",
    "test_features.sort_values(by=\"user\", inplace=True) #Sort by user\n",
    "\n",
    "# Select only important features\n",
    "model_features = joblib.load(\"model_features.pkl\")\n",
    "\n",
    "print(f\"test_features before selecting features {test_features.shape}\")\n",
    "\n",
    "# If feature does not exist, populate with 0s\n",
    "for feat in model_features:\n",
    "    if feat not in test_features.columns:\n",
    "        test_features[feat] = 0\n",
    "test_features = test_features[model_features]\n",
    "\n",
    "print(f\"test_features after selecting features {test_features.shape}\")\n",
    "\n",
    "\n",
    "print(test_features.columns)\n",
    "print(test_features.head())\n",
    "\n",
    "# Load scaler and polynomial transformer, and transform the test features\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "poly = joblib.load(\"poly.pkl\")\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "test_features_poly = poly.transform(test_features_scaled)\n",
    "\n",
    "# Load the trained model and predict probabilities (shape: #test_users x 6)\n",
    "xgb_model = joblib.load(\"model.pkl\")\n",
    "probabilities = xgb_model.predict_proba(test_features_poly)\n",
    "y_pred_proba_rf = xgb_model.predict_proba(test_features_poly)\n",
    "print(y_pred_proba_rf)\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save the predictions as an .npz file\n",
    "np.savez(\"predictions.npz\", probabilities=probabilities)\n",
    "print(f\"prediction shape {probabilities.shape}\")\n",
    "\n",
    "# View predictions.npz\n",
    "test_results=np.load(\"predictions.npz\")\n",
    "test_results_df = pd.DataFrame(test_results[\"probabilities\"])\n",
    "print(test_results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('predictions.npz')\n",
    "predictions = data['probabilities']\n",
    "\n",
    "class_counts = {i: 0 for i in range(6)}\n",
    "\n",
    "for row in predictions:\n",
    "    predicted_class = np.argmax(row)\n",
    "    class_counts[predicted_class] += 1\n",
    "\n",
    "print(\"Class instance counts:\")\n",
    "for class_label, count in class_counts.items():\n",
    "    print(f\"Class {class_label}: {count}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_true = test[\"y\"]\n",
    "\n",
    "# Convert true labels to a DataFrame\n",
    "df_y_true = pd.DataFrame(y_true, columns=[\"user\", \"true_label\"])\n",
    "\n",
    "# Load the predicted probabilities\n",
    "predictions_data = np.load(\"predictions.npz\")\n",
    "probabilities = predictions_data[\"probabilities\"]\n",
    "\n",
    "predicted_labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "df_predictions = pd.DataFrame({\n",
    "    \"user\": df_y_true[\"user\"],\n",
    "    \"true_label\": df_y_true[\"true_label\"],\n",
    "    \"predicted_label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Identify misclassified users\n",
    "df_predictions[\"correct\"] = df_predictions[\"true_label\"] == df_predictions[\"predicted_label\"]\n",
    "df_misclassified = df_predictions[df_predictions[\"correct\"] == False]\n",
    "# df_misclassified.head(2)\n",
    "# df_misclassified.to_csv(\"misclassified_users.csv\", index=False)\n",
    "\n",
    "auc_per_class = {}\n",
    "for i in range(probabilities.shape[1]):\n",
    "    binary_true = (df_predictions[\"true_label\"] == i).astype(int)\n",
    "    try:\n",
    "        auc = roc_auc_score(binary_true, probabilities[:, i])\n",
    "        auc_per_class[i] = auc\n",
    "        print(f\"  Class {i}: AUC = {auc:.3f}\")\n",
    "    except ValueError:\n",
    "        auc_per_class[i] = None\n",
    "\n",
    "k = 5\n",
    "AUC_0 = auc_per_class[0]\n",
    "anomaly_aucs = [auc_per_class[i] for i in range(1, k+1) if i in auc_per_class]\n",
    "\n",
    "final_metric = (0.5 * AUC_0) + (0.5 / k) * sum(anomaly_aucs)\n",
    "print(f\"\\nðŸ† Final Evaluation Metric: {final_metric:.3f}\")\n",
    "\n",
    "# Convert AUC scores to DataFrame\n",
    "df_auc = pd.DataFrame(list(auc_per_class.items()), columns=[\"class\", \"AUC\"])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CS421",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
