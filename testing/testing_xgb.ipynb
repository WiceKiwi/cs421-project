{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change the np.load to include the file\n",
    "- Add the following .pkl files into the same folder as this .ipynb (model, model_features, scaler, poly)\n",
    "- Edit the transform_features() method if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZrJYQUWBRE8",
    "outputId": "0992887d-734b-47ee-e5f7-72badc38ad9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique user IDs in the test set: 1100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "import joblib\n",
    "import networkx as nx\n",
    "\n",
    "test=np.load(\"../datasets/labeled/third_batch_multi_labels.npz\")\n",
    "\n",
    "X_test=test[\"X\"]\n",
    "\n",
    "XX_test = pd.DataFrame(X_test)\n",
    "XX_test.rename(columns={0:\"user\",1:\"item\",2:\"rating\"},inplace=True)\n",
    "\n",
    "num_unique_users = XX_test[\"user\"].nunique()\n",
    "print(f\"Number of unique user IDs in the test set: {num_unique_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2hprjtSwFBMA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "def transform_features(X):\n",
    "    \"\"\"\n",
    "    Transforms the input DataFrame X (assumed to have columns 'user', 'item', 'rating')\n",
    "    into a DataFrame with aggregated user-level features. These include:\n",
    "      - Aggregation of counts and summary statistics.\n",
    "      - Proportional features.\n",
    "      - Gap statistics on item IDs.\n",
    "      - Min, max, median, and variance of movie IDs.\n",
    "      - Product features based on item and rating.\n",
    "      - Rating distribution entropy.\n",
    "      - Movie popularity features.\n",
    "      - Unique movies count and diversity.\n",
    "      - Deviation from population average rating.\n",
    "      - Sequential pattern features (rating differences, etc.).\n",
    "      - Advanced interaction features and review count binning.\n",
    "\n",
    "    Returns a DataFrame with all the computed features.\n",
    "    \"\"\"\n",
    "    df = X.copy()\n",
    "\n",
    "    # --- AGGREGATION: Compute counts and summary statistics ---\n",
    "    user_features = df.groupby(\"user\").agg(\n",
    "        review_count=(\"rating\", \"count\"),\n",
    "        avg_rating=(\"rating\", \"mean\"),\n",
    "        std_rating=(\"rating\", \"std\"),\n",
    "        like_count=(\"rating\", lambda x: (x == 10).sum()),\n",
    "        dislike_count=(\"rating\", lambda x: (x == -10).sum()),\n",
    "        unknown_count=(\"rating\", lambda x: (x == 1).sum()),\n",
    "        neutral_count=(\"rating\", lambda x: (x == 0).sum()),\n",
    "    ).reset_index()\n",
    "\n",
    "    # --- PROPORTIONAL FEATURES ---\n",
    "    user_features[\"like_pct\"] = user_features[\"like_count\"] / (user_features[\"review_count\"] + EPS)\n",
    "    user_features[\"dislike_pct\"] = user_features[\"dislike_count\"] / (user_features[\"review_count\"] + EPS)\n",
    "    user_features[\"unknown_pct\"] = user_features[\"unknown_count\"] / (user_features[\"review_count\"] + EPS)\n",
    "    user_features[\"neutral_pct\"] = user_features[\"neutral_count\"] / (user_features[\"review_count\"] + EPS)\n",
    "\n",
    "    # --- GAP STATISTICS ---\n",
    "    XX_sorted = X.sort_values(by=['user', 'item'])\n",
    "    XX_sorted['item_diff'] = XX_sorted.groupby('user')['item'].diff().fillna(0)\n",
    "    gap_stats = XX_sorted.groupby('user')['item_diff'].agg(['mean', 'std', 'max', 'min'])\n",
    "    gap_stats.columns = ['gap_mean', 'gap_std', 'gap_max', 'gap_min']\n",
    "    user_features = user_features.merge(gap_stats, on='user', how='left')\n",
    "\n",
    "    # --- MIN/MAX/MEDIAN/VARIANCE OF MOVIE IDs ---\n",
    "    min_max_df = X.groupby(\"user\")[\"item\"].agg(min_movie=\"min\", max_movie=\"max\", median_movie=\"median\", variance_movie=\"var\").reset_index()\n",
    "    user_features = user_features.merge(min_max_df, on=\"user\", how=\"left\")\n",
    "\n",
    "    # --- PRODUCT FEATURES ---\n",
    "    X = X.copy()  # work on a copy to avoid modifying original data\n",
    "    X[\"item_rating\"] = X[\"item\"] * X[\"rating\"]\n",
    "    sum_rating = X.groupby(\"user\")[\"rating\"].sum().reset_index(name=\"sum_rating\")\n",
    "    sum_product = X.groupby(\"user\")[\"item_rating\"].sum().reset_index(name=\"sum_item_rating\")\n",
    "    user_features = user_features.merge(sum_product, on=\"user\", how=\"left\")\n",
    "    user_features = user_features.merge(sum_rating, on=\"user\", how=\"left\")\n",
    "\n",
    "    user_features[\"average_product\"] = user_features[\"sum_item_rating\"] / user_features[\"review_count\"]\n",
    "    user_features[\"product_above_zero\"] = (user_features[\"sum_item_rating\"] > 0).astype(int)\n",
    "    user_features[\"sum_above_zero\"] = (user_features[\"sum_rating\"] > 0).astype(int)\n",
    "    user_features[\"avg_product_vs_avg_rating\"] = user_features[\"average_product\"] / (user_features[\"avg_rating\"] + EPS)\n",
    "\n",
    "    # --- RATING DISTRIBUTION ENTROPY ---\n",
    "    def calc_entropy(row):\n",
    "        probs = [row[\"like_pct\"], row[\"dislike_pct\"], row[\"unknown_pct\"], row[\"neutral_pct\"]]\n",
    "        probs = [p for p in probs if p > 0]\n",
    "        return entropy(probs) if probs else 0\n",
    "    user_features[\"rating_entropy\"] = user_features.apply(calc_entropy, axis=1)\n",
    "\n",
    "    # --- MOVIE POPULARITY FEATURES ---\n",
    "    movie_popularity = X.groupby(\"item\").size().reset_index(name=\"movie_popularity\")\n",
    "    X_with_pop = X.merge(movie_popularity, on=\"item\")\n",
    "    pop_features = X_with_pop.groupby(\"user\").agg(\n",
    "        avg_movie_popularity=(\"movie_popularity\", \"mean\"),\n",
    "        std_movie_popularity=(\"movie_popularity\", \"std\"),\n",
    "        min_movie_popularity=(\"movie_popularity\", \"min\"),\n",
    "        max_movie_popularity=(\"movie_popularity\", \"max\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Define movie popularity percentiles\n",
    "    movie_popularity = X['item'].value_counts().reset_index()\n",
    "    movie_popularity.columns = ['item', 'popularity']\n",
    "    threshold = movie_popularity['popularity'].quantile(0.1)\n",
    "    rare_movies = movie_popularity[movie_popularity['popularity'] <= threshold]['item'].values\n",
    "    \n",
    "    X['is_rare_movie'] = X['item'].isin(rare_movies).astype(int)\n",
    "    \n",
    "    rare_stats = X.groupby('user')['is_rare_movie'].mean().reset_index(name='rare_movies_watched_pct')\n",
    "    user_features = user_features.merge(rare_stats, on='user', how='left')\n",
    "\n",
    "    # --- UNIQUE MOVIES AND DIVERSITY ---\n",
    "    unique_items = X.groupby(\"user\")[\"item\"].nunique().reset_index()\n",
    "    unique_items.columns = [\"user\", \"unique_movies\"]\n",
    "\n",
    "    # --- DEVIATION FROM POPULATION FEATURES ---\n",
    "    movie_avg_rating = X.groupby(\"item\")[\"rating\"].mean().reset_index(name=\"movie_avg_rating\")\n",
    "    X_with_avg = X.merge(movie_avg_rating, on=\"item\")\n",
    "    X_with_avg[\"rating_deviation\"] = X_with_avg[\"rating\"] - X_with_avg[\"movie_avg_rating\"]\n",
    "    X_with_avg[\"abs_rating_deviation\"] = np.abs(X_with_avg[\"rating_deviation\"])\n",
    "    deviation_features = X_with_avg.groupby(\"user\").agg(\n",
    "        mean_deviation=(\"rating_deviation\", \"mean\"),\n",
    "        std_deviation=(\"rating_deviation\", \"std\"),\n",
    "        mean_abs_deviation=(\"abs_rating_deviation\", \"mean\"),\n",
    "        max_abs_deviation=(\"abs_rating_deviation\", \"max\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # --- SEQUENTIAL PATTERN FEATURES ---\n",
    "    X_sorted = X.sort_values([\"user\", \"item\"])\n",
    "    X_sorted[\"next_rating\"] = X_sorted.groupby(\"user\")[\"rating\"].shift(-1)\n",
    "    X_sorted[\"rating_diff\"] = X_sorted[\"next_rating\"] - X_sorted[\"rating\"]\n",
    "    X_sorted[\"abs_rating_diff\"] = np.abs(X_sorted[\"rating_diff\"])\n",
    "    X_sorted = X_sorted.dropna(subset=[\"rating_diff\"])\n",
    "    sequence_features = X_sorted.groupby(\"user\").agg(\n",
    "        mean_rating_diff=(\"rating_diff\", \"mean\"),\n",
    "        std_rating_diff=(\"rating_diff\", \"std\"),\n",
    "        mean_abs_rating_diff=(\"abs_rating_diff\", \"mean\"),\n",
    "        max_abs_rating_diff=(\"abs_rating_diff\", \"max\"),\n",
    "        rating_changes_count=(\"rating_diff\", lambda x: (x != 0).sum()),\n",
    "    ).reset_index()\n",
    "    sequence_features[\"rating_changes_pct\"] = sequence_features[\"rating_changes_count\"] / (\n",
    "        user_features.set_index(\"user\")[\"review_count\"] - 1 + EPS\n",
    "    ).reindex(sequence_features[\"user\"]).values\n",
    "\n",
    "    X_sorted[\"rating_direction\"] = X_sorted[\"rating_diff\"].apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    X_sorted['direction_switch'] = X_sorted['rating_direction'] != X_sorted.groupby(\"user\")['rating_direction'].shift(1)\n",
    "    switch_count = X_sorted.groupby(\"user\")[\"direction_switch\"].sum().reset_index(name=\"change_direction_count\")\n",
    "    user_features = user_features.merge(switch_count, on=\"user\", how=\"left\")\n",
    "\n",
    "    # --- COMBINE ALL FEATURES ---\n",
    "    all_features = user_features.merge(pop_features, on=\"user\", how=\"left\")\n",
    "    all_features = all_features.merge(unique_items, on=\"user\", how=\"left\")\n",
    "    all_features = all_features.merge(deviation_features, on=\"user\", how=\"left\")\n",
    "    all_features = all_features.merge(sequence_features, on=\"user\", how=\"left\")\n",
    "\n",
    "    all_features = all_features.fillna(0)\n",
    "\n",
    "    # --- UNSUPERVISED ANOMALY DETECTION FEATURES ---\n",
    "    feature_cols = [col for col in all_features.columns\n",
    "                    if col not in [\"user\", \"label\", \"is_anomalous\"]\n",
    "                    and all_features[col].dtype in [np.float64, np.int64]]\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(all_features[feature_cols])\n",
    "\n",
    "    # --- ADVANCED INTERACTION FEATURES ---\n",
    "    all_features[\"like_dislike_ratio\"] = all_features[\"like_count\"] / (all_features[\"dislike_count\"] + EPS)\n",
    "    all_features[\"rating_range\"] = all_features[\"max_abs_rating_diff\"]\n",
    "    all_features[\"popularity_vs_deviation\"] = all_features[\"avg_movie_popularity\"] * all_features[\"mean_abs_deviation\"]\n",
    "    all_features[\"entropy_by_count\"] = all_features[\"rating_entropy\"] * np.log1p(all_features[\"review_count\"])\n",
    "\n",
    "    # --- BINNING FEATURES ---\n",
    "    all_features[\"review_count_bin\"] = pd.qcut(all_features[\"review_count\"], q=5, labels=False, duplicates=\"drop\")\n",
    "\n",
    "    return pd.DataFrame(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRt-PQQ4Biv6",
    "outputId": "6a93053f-949f-462c-ff0e-5bce3e2fa995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_features before selecting features (1100, 49)\n",
      "test_features after selecting features (1100, 36)\n",
      "Index(['review_count', 'avg_rating', 'std_rating', 'like_count',\n",
      "       'dislike_count', 'neutral_count', 'dislike_pct', 'unknown_pct',\n",
      "       'neutral_pct', 'rating_entropy', 'rare_movies_watched_pct',\n",
      "       'avg_movie_popularity', 'std_movie_popularity', 'min_movie_popularity',\n",
      "       'max_movie_popularity', 'std_deviation', 'mean_abs_deviation',\n",
      "       'max_abs_deviation', 'mean_rating_diff', 'std_rating_diff',\n",
      "       'max_abs_rating_diff', 'rating_changes_pct', 'like_dislike_ratio',\n",
      "       'popularity_vs_deviation', 'entropy_by_count', 'min_movie', 'max_movie',\n",
      "       'median_movie', 'variance_movie', 'sum_item_rating', 'average_product',\n",
      "       'product_above_zero', 'sum_above_zero', 'avg_product_vs_avg_rating',\n",
      "       'gap_mean', 'gap_max'],\n",
      "      dtype='object')\n",
      "   review_count  avg_rating  std_rating  like_count  dislike_count  \\\n",
      "0           213    3.408451    5.135643          73              7   \n",
      "1           141    2.127660    6.325516          44             18   \n",
      "2           278   -1.183453    7.129955          52             92   \n",
      "3            34    3.323529    6.123579          13              3   \n",
      "4           150    4.926667    5.500576          75              6   \n",
      "\n",
      "   neutral_count  dislike_pct  unknown_pct  neutral_pct  rating_entropy  ...  \\\n",
      "0             67     0.032864     0.309859     0.314554        1.206097  ...   \n",
      "1             39     0.127660     0.283688     0.276596        1.339078  ...   \n",
      "2             63     0.330935     0.255396     0.226619        1.364538  ...   \n",
      "3              5     0.088235     0.382353     0.147059        1.231310  ...   \n",
      "4             20     0.040000     0.326667     0.133333        1.109462  ...   \n",
      "\n",
      "   max_movie  median_movie  variance_movie  sum_item_rating  average_product  \\\n",
      "0        990         610.0    91183.884401           377207      1770.924883   \n",
      "1        982         412.0    74704.736272           175864      1247.262411   \n",
      "2        992         566.0    84406.240864          -240153      -863.859712   \n",
      "3        939         416.5    67544.528520            54271      1596.205882   \n",
      "4        613         300.5    29999.932170           199537      1330.246667   \n",
      "\n",
      "   product_above_zero  sum_above_zero  avg_product_vs_avg_rating   gap_mean  \\\n",
      "0                   1               1                 519.568718   4.647887   \n",
      "1                   1               1                 586.213058   6.914894   \n",
      "2                   0               0                 729.948945   3.568345   \n",
      "3                   1               1                 480.274192  27.147059   \n",
      "4                   1               1                 270.009417   4.086667   \n",
      "\n",
      "   gap_max  \n",
      "0     33.0  \n",
      "1     51.0  \n",
      "2     28.0  \n",
      "3     74.0  \n",
      "4     34.0  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "[[9.91292655e-01 8.77034618e-05 1.30535965e-03 2.05575870e-05\n",
      "  2.03143083e-03 5.26223565e-03]\n",
      " [9.99053657e-01 2.53451988e-04 8.15817548e-05 1.25134484e-05\n",
      "  3.73590970e-04 2.25162090e-04]\n",
      " [8.67722392e-01 6.05122838e-03 9.08039685e-04 6.09986950e-04\n",
      "  1.21236585e-01 3.47179803e-03]\n",
      " ...\n",
      " [9.95695472e-01 1.49099354e-03 1.40877499e-04 1.78764603e-04\n",
      "  1.72593398e-03 7.67947233e-04]\n",
      " [9.91218626e-01 1.98895531e-03 2.07392673e-04 4.91357932e-04\n",
      "  6.35588251e-04 5.45805926e-03]\n",
      " [9.96221781e-01 1.13741285e-03 1.71063861e-04 1.01871010e-04\n",
      "  1.26322859e-03 1.10456359e-03]]\n",
      "prediction shape (1100, 6)\n",
      "          0         1         2         3         4         5\n",
      "0  0.991293  0.000088  0.001305  0.000021  0.002031  0.005262\n",
      "1  0.999054  0.000253  0.000082  0.000013  0.000374  0.000225\n",
      "2  0.867722  0.006051  0.000908  0.000610  0.121237  0.003472\n",
      "3  0.994966  0.003493  0.000402  0.000087  0.000570  0.000482\n",
      "4  0.997732  0.001122  0.000175  0.000012  0.000057  0.000901\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering\n",
    "test_features = transform_features(XX_test)\n",
    "test_features \n",
    "test_features.sort_values(by=\"user\", inplace=True) #Sort by user\n",
    "\n",
    "# Select only important features\n",
    "model_features = joblib.load(\"model_features.pkl\")\n",
    "\n",
    "print(f\"test_features before selecting features {test_features.shape}\")\n",
    "\n",
    "# If feature does not exist, populate with 0s\n",
    "for feat in model_features:\n",
    "    if feat not in test_features.columns:\n",
    "        test_features[feat] = 0\n",
    "test_features = test_features[model_features]\n",
    "\n",
    "print(f\"test_features after selecting features {test_features.shape}\")\n",
    "\n",
    "\n",
    "print(test_features.columns)\n",
    "print(test_features.head())\n",
    "\n",
    "# Load scaler and polynomial transformer, and transform the test features\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "poly = joblib.load(\"poly.pkl\")\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "test_features_poly = poly.transform(test_features_scaled)\n",
    "\n",
    "# Load the trained model and predict probabilities (shape: #test_users x 6)\n",
    "xgb_model = joblib.load(\"model.pkl\")\n",
    "probabilities = xgb_model.predict_proba(test_features_poly)\n",
    "y_pred_proba_rf = xgb_model.predict_proba(test_features_poly)\n",
    "print(y_pred_proba_rf)\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save the predictions as an .npz file\n",
    "np.savez(\"predictions.npz\", probabilities=probabilities)\n",
    "print(f\"prediction shape {probabilities.shape}\")\n",
    "\n",
    "# View predictions.npz\n",
    "test_results=np.load(\"predictions.npz\")\n",
    "test_results_df = pd.DataFrame(test_results[\"probabilities\"])\n",
    "print(test_results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class instance counts:\n",
      "Class 0: 994\n",
      "Class 1: 15\n",
      "Class 2: 7\n",
      "Class 3: 21\n",
      "Class 4: 21\n",
      "Class 5: 42\n"
     ]
    }
   ],
   "source": [
    "data = np.load('predictions.npz')\n",
    "predictions = data['probabilities']\n",
    "\n",
    "class_counts = {i: 0 for i in range(6)}\n",
    "\n",
    "for row in predictions:\n",
    "    predicted_class = np.argmax(row)\n",
    "    class_counts[predicted_class] += 1\n",
    "\n",
    "print(\"Class instance counts:\")\n",
    "for class_label, count in class_counts.items():\n",
    "    print(f\"Class {class_label}: {count}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 0: AUC = 0.862\n",
      "  Class 1: AUC = 0.794\n",
      "  Class 2: AUC = 0.818\n",
      "  Class 3: AUC = 0.984\n",
      "  Class 4: AUC = 0.719\n",
      "  Class 5: AUC = 0.705\n",
      "\n",
      "ðŸ† Final Evaluation Metric: 0.833\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_true = test[\"y\"]\n",
    "\n",
    "# Convert true labels to a DataFrame\n",
    "df_y_true = pd.DataFrame(y_true, columns=[\"user\", \"true_label\"])\n",
    "\n",
    "# Load the predicted probabilities\n",
    "predictions_data = np.load(\"predictions.npz\")\n",
    "probabilities = predictions_data[\"probabilities\"]\n",
    "\n",
    "predicted_labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "df_predictions = pd.DataFrame({\n",
    "    \"user\": df_y_true[\"user\"],\n",
    "    \"true_label\": df_y_true[\"true_label\"],\n",
    "    \"predicted_label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Identify misclassified users\n",
    "df_predictions[\"correct\"] = df_predictions[\"true_label\"] == df_predictions[\"predicted_label\"]\n",
    "df_misclassified = df_predictions[df_predictions[\"correct\"] == False]\n",
    "# df_misclassified.head(2)\n",
    "# df_misclassified.to_csv(\"misclassified_users.csv\", index=False)\n",
    "\n",
    "auc_per_class = {}\n",
    "for i in range(probabilities.shape[1]):\n",
    "    binary_true = (df_predictions[\"true_label\"] == i).astype(int)\n",
    "    try:\n",
    "        auc = roc_auc_score(binary_true, probabilities[:, i])\n",
    "        auc_per_class[i] = auc\n",
    "        print(f\"  Class {i}: AUC = {auc:.3f}\")\n",
    "    except ValueError:\n",
    "        auc_per_class[i] = None\n",
    "\n",
    "k = 5\n",
    "AUC_0 = auc_per_class[0]\n",
    "anomaly_aucs = [auc_per_class[i] for i in range(1, k+1) if i in auc_per_class]\n",
    "\n",
    "final_metric = (0.5 * AUC_0) + (0.5 / k) * sum(anomaly_aucs)\n",
    "print(f\"\\nðŸ† Final Evaluation Metric: {final_metric:.3f}\")\n",
    "\n",
    "# Convert AUC scores to DataFrame\n",
    "df_auc = pd.DataFrame(list(auc_per_class.items()), columns=[\"class\", \"AUC\"])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CS421",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
