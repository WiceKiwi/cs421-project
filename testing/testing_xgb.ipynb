{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change the np.load to include the file\n",
    "- Add the following .pkl files into the same folder as this .ipynb (model, model_features, scaler, poly)\n",
    "- Edit the transform_features() method if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZrJYQUWBRE8",
    "outputId": "0992887d-734b-47ee-e5f7-72badc38ad9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique user IDs in the test set: 1100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "import joblib\n",
    "import networkx as nx\n",
    "\n",
    "test=np.load(\"../datasets/labeled/third_batch_multi.npz\")\n",
    "\n",
    "X_test=test[\"X\"]\n",
    "\n",
    "XX_test = pd.DataFrame(X_test)\n",
    "XX_test.rename(columns={0:\"user\",1:\"item\",2:\"rating\"},inplace=True)\n",
    "\n",
    "num_unique_users = XX_test[\"user\"].nunique()\n",
    "print(f\"Number of unique user IDs in the test set: {num_unique_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2hprjtSwFBMA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "def transform_features(X):\n",
    "    \"\"\"\n",
    "    Transforms the input DataFrame X (assumed to have columns 'user', 'item', 'rating')\n",
    "    into a DataFrame with aggregated user-level features. These include:\n",
    "      - Aggregation of counts and summary statistics.\n",
    "      - Proportional features.\n",
    "      - Gap statistics on item IDs.\n",
    "      - Min, max, median, and variance of movie IDs.\n",
    "      - Product features based on item and rating.\n",
    "      - Rating distribution entropy.\n",
    "      - Movie popularity features.\n",
    "      - Unique movies count and diversity.\n",
    "      - Deviation from population average rating.\n",
    "      - Sequential pattern features (rating differences, etc.).\n",
    "      - Advanced interaction features and review count binning.\n",
    "\n",
    "    Returns a DataFrame with all the computed features.\n",
    "    \"\"\"\n",
    "    df = X.copy()\n",
    "\n",
    "    # --- AGGREGATION: Compute counts and summary statistics ---\n",
    "    user_features = df.groupby(\"user\").agg(\n",
    "        review_count=(\"rating\", \"count\"),\n",
    "        avg_rating=(\"rating\", \"mean\"),\n",
    "        std_rating=(\"rating\", \"std\"),\n",
    "        like_count=(\"rating\", lambda x: (x == 10).sum()),\n",
    "        dislike_count=(\"rating\", lambda x: (x == -10).sum()),\n",
    "        unknown_count=(\"rating\", lambda x: (x == 1).sum()),\n",
    "        neutral_count=(\"rating\", lambda x: (x == 0).sum()),\n",
    "    ).reset_index()\n",
    "\n",
    "    # --- PROPORTIONAL FEATURES ---\n",
    "    user_features[\"like_pct\"] = user_features[\"like_count\"] / (user_features[\"review_count\"] + EPS)\n",
    "    user_features[\"dislike_pct\"] = user_features[\"dislike_count\"] / (user_features[\"review_count\"] + EPS)\n",
    "    user_features[\"unknown_pct\"] = user_features[\"unknown_count\"] / (user_features[\"review_count\"] + EPS)\n",
    "    user_features[\"neutral_pct\"] = user_features[\"neutral_count\"] / (user_features[\"review_count\"] + EPS)\n",
    "\n",
    "    # --- GAP STATISTICS ---\n",
    "    XX_sorted = X.sort_values(by=['user', 'item'])\n",
    "    XX_sorted['item_diff'] = XX_sorted.groupby('user')['item'].diff().fillna(0)\n",
    "    gap_stats = XX_sorted.groupby('user')['item_diff'].agg(['mean', 'std', 'max', 'min'])\n",
    "    gap_stats.columns = ['gap_mean', 'gap_std', 'gap_max', 'gap_min']\n",
    "    user_features = user_features.merge(gap_stats, on='user', how='left')\n",
    "\n",
    "    # --- MIN/MAX/MEDIAN/VARIANCE OF MOVIE IDs ---\n",
    "    min_max_df = X.groupby(\"user\")[\"item\"].agg(min_movie=\"min\", max_movie=\"max\", median_movie=\"median\", variance_movie=\"var\").reset_index()\n",
    "    user_features = user_features.merge(min_max_df, on=\"user\", how=\"left\")\n",
    "\n",
    "    # --- PRODUCT FEATURES ---\n",
    "    X = X.copy()  # work on a copy to avoid modifying original data\n",
    "    X[\"item_rating\"] = X[\"item\"] * X[\"rating\"]\n",
    "    sum_rating = X.groupby(\"user\")[\"rating\"].sum().reset_index(name=\"sum_rating\")\n",
    "    sum_product = X.groupby(\"user\")[\"item_rating\"].sum().reset_index(name=\"sum_item_rating\")\n",
    "    user_features = user_features.merge(sum_product, on=\"user\", how=\"left\")\n",
    "    user_features = user_features.merge(sum_rating, on=\"user\", how=\"left\")\n",
    "\n",
    "    user_features[\"average_product\"] = user_features[\"sum_item_rating\"] / user_features[\"review_count\"]\n",
    "    user_features[\"product_above_zero\"] = (user_features[\"sum_item_rating\"] > 0).astype(int)\n",
    "    user_features[\"sum_above_zero\"] = (user_features[\"sum_rating\"] > 0).astype(int)\n",
    "    user_features[\"avg_product_vs_avg_rating\"] = user_features[\"average_product\"] / (user_features[\"avg_rating\"] + EPS)\n",
    "\n",
    "    # --- RATING DISTRIBUTION ENTROPY ---\n",
    "    def calc_entropy(row):\n",
    "        probs = [row[\"like_pct\"], row[\"dislike_pct\"], row[\"unknown_pct\"], row[\"neutral_pct\"]]\n",
    "        probs = [p for p in probs if p > 0]\n",
    "        return entropy(probs) if probs else 0\n",
    "    user_features[\"rating_entropy\"] = user_features.apply(calc_entropy, axis=1)\n",
    "\n",
    "    # --- MOVIE POPULARITY FEATURES ---\n",
    "    movie_popularity = X.groupby(\"item\").size().reset_index(name=\"movie_popularity\")\n",
    "    X_with_pop = X.merge(movie_popularity, on=\"item\")\n",
    "    pop_features = X_with_pop.groupby(\"user\").agg(\n",
    "        avg_movie_popularity=(\"movie_popularity\", \"mean\"),\n",
    "        std_movie_popularity=(\"movie_popularity\", \"std\"),\n",
    "        min_movie_popularity=(\"movie_popularity\", \"min\"),\n",
    "        max_movie_popularity=(\"movie_popularity\", \"max\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # --- UNIQUE MOVIES AND DIVERSITY ---\n",
    "    unique_items = X.groupby(\"user\")[\"item\"].nunique().reset_index()\n",
    "    unique_items.columns = [\"user\", \"unique_movies\"]\n",
    "\n",
    "    # --- DEVIATION FROM POPULATION FEATURES ---\n",
    "    movie_avg_rating = X.groupby(\"item\")[\"rating\"].mean().reset_index(name=\"movie_avg_rating\")\n",
    "    X_with_avg = X.merge(movie_avg_rating, on=\"item\")\n",
    "    X_with_avg[\"rating_deviation\"] = X_with_avg[\"rating\"] - X_with_avg[\"movie_avg_rating\"]\n",
    "    X_with_avg[\"abs_rating_deviation\"] = np.abs(X_with_avg[\"rating_deviation\"])\n",
    "    deviation_features = X_with_avg.groupby(\"user\").agg(\n",
    "        mean_deviation=(\"rating_deviation\", \"mean\"),\n",
    "        std_deviation=(\"rating_deviation\", \"std\"),\n",
    "        mean_abs_deviation=(\"abs_rating_deviation\", \"mean\"),\n",
    "        max_abs_deviation=(\"abs_rating_deviation\", \"max\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # --- SEQUENTIAL PATTERN FEATURES ---\n",
    "    X_sorted = X.sort_values([\"user\", \"item\"])\n",
    "    X_sorted[\"next_rating\"] = X_sorted.groupby(\"user\")[\"rating\"].shift(-1)\n",
    "    X_sorted[\"rating_diff\"] = X_sorted[\"next_rating\"] - X_sorted[\"rating\"]\n",
    "    X_sorted[\"abs_rating_diff\"] = np.abs(X_sorted[\"rating_diff\"])\n",
    "    X_sorted = X_sorted.dropna(subset=[\"rating_diff\"])\n",
    "    sequence_features = X_sorted.groupby(\"user\").agg(\n",
    "        mean_rating_diff=(\"rating_diff\", \"mean\"),\n",
    "        std_rating_diff=(\"rating_diff\", \"std\"),\n",
    "        mean_abs_rating_diff=(\"abs_rating_diff\", \"mean\"),\n",
    "        max_abs_rating_diff=(\"abs_rating_diff\", \"max\"),\n",
    "        rating_changes_count=(\"rating_diff\", lambda x: (x != 0).sum()),\n",
    "    ).reset_index()\n",
    "    sequence_features[\"rating_changes_pct\"] = sequence_features[\"rating_changes_count\"] / (\n",
    "        user_features.set_index(\"user\")[\"review_count\"] - 1 + EPS\n",
    "    ).reindex(sequence_features[\"user\"]).values\n",
    "\n",
    "    # --- COMBINE ALL FEATURES ---\n",
    "    all_features = user_features.merge(pop_features, on=\"user\", how=\"left\")\n",
    "    all_features = all_features.merge(unique_items, on=\"user\", how=\"left\")\n",
    "    all_features = all_features.merge(deviation_features, on=\"user\", how=\"left\")\n",
    "    all_features = all_features.merge(sequence_features, on=\"user\", how=\"left\")\n",
    "\n",
    "    all_features = all_features.fillna(0)\n",
    "\n",
    "    # --- UNSUPERVISED ANOMALY DETECTION FEATURES ---\n",
    "    feature_cols = [col for col in all_features.columns\n",
    "                    if col not in [\"user\", \"label\", \"is_anomalous\"]\n",
    "                    and all_features[col].dtype in [np.float64, np.int64]]\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(all_features[feature_cols])\n",
    "\n",
    "    # --- ADVANCED INTERACTION FEATURES ---\n",
    "    all_features[\"like_dislike_ratio\"] = all_features[\"like_count\"] / (all_features[\"dislike_count\"] + EPS)\n",
    "    all_features[\"rating_range\"] = all_features[\"max_abs_rating_diff\"]\n",
    "    all_features[\"popularity_vs_deviation\"] = all_features[\"avg_movie_popularity\"] * all_features[\"mean_abs_deviation\"]\n",
    "    all_features[\"entropy_by_count\"] = all_features[\"rating_entropy\"] * np.log1p(all_features[\"review_count\"])\n",
    "\n",
    "    # --- BINNING FEATURES ---\n",
    "    all_features[\"review_count_bin\"] = pd.qcut(all_features[\"review_count\"], q=5, labels=False, duplicates=\"drop\")\n",
    "\n",
    "    return pd.DataFrame(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRt-PQQ4Biv6",
    "outputId": "6a93053f-949f-462c-ff0e-5bce3e2fa995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_features before selecting features (1100, 47)\n",
      "test_features after selecting features (1100, 35)\n",
      "Index(['review_count', 'avg_rating', 'std_rating', 'like_count',\n",
      "       'dislike_count', 'neutral_count', 'dislike_pct', 'unknown_pct',\n",
      "       'neutral_pct', 'rating_entropy', 'avg_movie_popularity',\n",
      "       'std_movie_popularity', 'min_movie_popularity', 'max_movie_popularity',\n",
      "       'std_deviation', 'mean_abs_deviation', 'max_abs_deviation',\n",
      "       'mean_rating_diff', 'std_rating_diff', 'max_abs_rating_diff',\n",
      "       'rating_changes_pct', 'like_dislike_ratio', 'popularity_vs_deviation',\n",
      "       'entropy_by_count', 'min_movie', 'max_movie', 'median_movie',\n",
      "       'variance_movie', 'sum_item_rating', 'average_product',\n",
      "       'product_above_zero', 'sum_above_zero', 'avg_product_vs_avg_rating',\n",
      "       'gap_mean', 'gap_max'],\n",
      "      dtype='object')\n",
      "   review_count  avg_rating  std_rating  like_count  dislike_count  \\\n",
      "0           213    3.408451    5.135643          73              7   \n",
      "1           141    2.127660    6.325516          44             18   \n",
      "2           278   -1.183453    7.129955          52             92   \n",
      "3            34    3.323529    6.123579          13              3   \n",
      "4           150    4.926667    5.500576          75              6   \n",
      "\n",
      "   neutral_count  dislike_pct  unknown_pct  neutral_pct  rating_entropy  ...  \\\n",
      "0             67     0.032864     0.309859     0.314554        1.206097  ...   \n",
      "1             39     0.127660     0.283688     0.276596        1.339078  ...   \n",
      "2             63     0.330935     0.255396     0.226619        1.364538  ...   \n",
      "3              5     0.088235     0.382353     0.147059        1.231310  ...   \n",
      "4             20     0.040000     0.326667     0.133333        1.109462  ...   \n",
      "\n",
      "   max_movie  median_movie  variance_movie  sum_item_rating  average_product  \\\n",
      "0        990         610.0    91183.884401           377207      1770.924883   \n",
      "1        982         412.0    74704.736272           175864      1247.262411   \n",
      "2        992         566.0    84406.240864          -240153      -863.859712   \n",
      "3        939         416.5    67544.528520            54271      1596.205882   \n",
      "4        613         300.5    29999.932170           199537      1330.246667   \n",
      "\n",
      "   product_above_zero  sum_above_zero  avg_product_vs_avg_rating   gap_mean  \\\n",
      "0                   1               1                 519.568718   4.647887   \n",
      "1                   1               1                 586.213058   6.914894   \n",
      "2                   0               0                 729.948945   3.568345   \n",
      "3                   1               1                 480.274192  27.147059   \n",
      "4                   1               1                 270.009417   4.086667   \n",
      "\n",
      "   gap_max  \n",
      "0     33.0  \n",
      "1     51.0  \n",
      "2     28.0  \n",
      "3     74.0  \n",
      "4     34.0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "[[9.8436654e-01 4.0361562e-04 4.6086083e-03 3.9574195e-05 3.6042703e-03\n",
      "  6.9774110e-03]\n",
      " [9.6531940e-01 7.9307184e-03 6.8665547e-03 2.7339146e-04 1.5284666e-02\n",
      "  4.3252646e-03]\n",
      " [6.4390790e-01 1.7334033e-02 1.9339926e-03 5.2477320e-04 3.3354020e-01\n",
      "  2.7591111e-03]\n",
      " ...\n",
      " [9.9821389e-01 3.9162984e-04 5.8935009e-05 1.9015966e-04 5.0755107e-04\n",
      "  6.3791889e-04]\n",
      " [9.8985928e-01 2.3275330e-03 5.2254723e-04 1.4601579e-03 7.9942099e-04\n",
      "  5.0310129e-03]\n",
      " [9.9599910e-01 3.6650035e-04 9.4427975e-05 1.9359699e-04 6.4813392e-04\n",
      "  2.6982825e-03]]\n",
      "prediction shape (1100, 6)\n",
      "          0         1         2         3         4         5\n",
      "0  0.984367  0.000404  0.004609  0.000040  0.003604  0.006977\n",
      "1  0.965319  0.007931  0.006867  0.000273  0.015285  0.004325\n",
      "2  0.643908  0.017334  0.001934  0.000525  0.333540  0.002759\n",
      "3  0.991264  0.006160  0.000281  0.000225  0.001596  0.000473\n",
      "4  0.994848  0.001186  0.000050  0.000021  0.000149  0.003745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator PolynomialFeatures from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [16:29:56] WARNING: /Users/runner/work/xgboost/xgboost/src/gbm/../common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering\n",
    "test_features = transform_features(XX_test)\n",
    "test_features \n",
    "test_features.sort_values(by=\"user\", inplace=True) #Sort by user\n",
    "\n",
    "# Select only important features\n",
    "model_features = joblib.load(\"model_features.pkl\")\n",
    "\n",
    "print(f\"test_features before selecting features {test_features.shape}\")\n",
    "\n",
    "# If feature does not exist, populate with 0s\n",
    "for feat in model_features:\n",
    "    if feat not in test_features.columns:\n",
    "        test_features[feat] = 0\n",
    "test_features = test_features[model_features]\n",
    "\n",
    "print(f\"test_features after selecting features {test_features.shape}\")\n",
    "\n",
    "\n",
    "print(test_features.columns)\n",
    "print(test_features.head())\n",
    "\n",
    "# Load scaler and polynomial transformer, and transform the test features\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "poly = joblib.load(\"poly.pkl\")\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "test_features_poly = poly.transform(test_features_scaled)\n",
    "\n",
    "# Load the trained model and predict probabilities (shape: #test_users x 6)\n",
    "xgb_model = joblib.load(\"model.pkl\")\n",
    "probabilities = xgb_model.predict_proba(test_features_poly)\n",
    "y_pred_proba_rf = xgb_model.predict_proba(test_features_poly)\n",
    "print(y_pred_proba_rf)\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save the predictions as an .npz file\n",
    "np.savez(\"predictions.npz\", probabilities=probabilities)\n",
    "print(f\"prediction shape {probabilities.shape}\")\n",
    "\n",
    "# View predictions.npz\n",
    "test_results=np.load(\"predictions.npz\")\n",
    "test_results_df = pd.DataFrame(test_results[\"probabilities\"])\n",
    "print(test_results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class instance counts:\n",
      "Class 0: 972\n",
      "Class 1: 16\n",
      "Class 2: 14\n",
      "Class 3: 25\n",
      "Class 4: 34\n",
      "Class 5: 39\n"
     ]
    }
   ],
   "source": [
    "data = np.load('predictions.npz')\n",
    "predictions = data['probabilities']  # Assuming the array is stored under 'arr_0'\n",
    "\n",
    "class_counts = {i: 0 for i in range(6)}  # Initialize counts for each class\n",
    "\n",
    "for row in predictions:\n",
    "    predicted_class = np.argmax(row)  # Get the class with the highest probability\n",
    "    class_counts[predicted_class] += 1\n",
    "\n",
    "print(\"Class instance counts:\")\n",
    "for class_label, count in class_counts.items():\n",
    "    print(f\"Class {class_label}: {count}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 0: AUC = 0.861\n",
      "  Class 1: AUC = 0.756\n",
      "  Class 2: AUC = 0.834\n",
      "  Class 3: AUC = 0.975\n",
      "  Class 4: AUC = 0.766\n",
      "  Class 5: AUC = 0.699\n",
      "\n",
      "üèÜ Final Evaluation Metric: 0.834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_true = test[\"y\"]  # True labels\n",
    "\n",
    "# Convert true labels to a DataFrame\n",
    "df_y_true = pd.DataFrame(y_true, columns=[\"user\", \"true_label\"])\n",
    "\n",
    "# Load the predicted probabilities\n",
    "predictions_data = np.load(\"predictions.npz\")\n",
    "probabilities = predictions_data[\"probabilities\"]\n",
    "\n",
    "# Get the predicted class (argmax of each row)\n",
    "predicted_labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Create a DataFrame for predictions\n",
    "df_predictions = pd.DataFrame({\n",
    "    \"user\": df_y_true[\"user\"],  # Assuming user IDs match in order\n",
    "    \"true_label\": df_y_true[\"true_label\"],\n",
    "    \"predicted_label\": predicted_labels\n",
    "})\n",
    "\n",
    "# Identify misclassified users\n",
    "df_predictions[\"correct\"] = df_predictions[\"true_label\"] == df_predictions[\"predicted_label\"]\n",
    "df_misclassified = df_predictions[df_predictions[\"correct\"] == False]\n",
    "# Display the comparison\n",
    "# tools.display_dataframe_to_user(name=\"True vs Predicted Labels\", dataframe=df_predictions)\n",
    "df_misclassified\n",
    "df_misclassified.to_csv(\"misclassified_users.csv\", index=False)\n",
    "\n",
    "# Compute AUC for each class\n",
    "auc_per_class = {}\n",
    "for i in range(probabilities.shape[1]):  # Iterate over all classes\n",
    "    binary_true = (df_predictions[\"true_label\"] == i).astype(int)\n",
    "    try:\n",
    "        auc = roc_auc_score(binary_true, probabilities[:, i])\n",
    "        auc_per_class[i] = auc\n",
    "        print(f\"  Class {i}: AUC = {auc:.3f}\")\n",
    "    except ValueError:\n",
    "        auc_per_class[i] = None  # If AUC cannot be computed (e.g., only one class present)\n",
    "\n",
    "k = 5\n",
    "AUC_0 = auc_per_class[0]\n",
    "anomaly_aucs = [auc_per_class[i] for i in range(1, k+1) if i in auc_per_class]\n",
    "\n",
    "final_metric = (0.5 * AUC_0) + (0.5 / k) * sum(anomaly_aucs)\n",
    "print(f\"\\nüèÜ Final Evaluation Metric: {final_metric:.3f}\")\n",
    "\n",
    "# Convert AUC scores to DataFrame\n",
    "df_auc = pd.DataFrame(list(auc_per_class.items()), columns=[\"class\", \"AUC\"])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
