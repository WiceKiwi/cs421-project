{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Change the np.load to include the file\n",
        "- Add the following .pkl files into the same folder as this .ipynb (model, model_features, scaler, poly)\n",
        "- Edit the transform_features() method if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZrJYQUWBRE8",
        "outputId": "0992887d-734b-47ee-e5f7-72badc38ad9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\n",
            "0    0\n",
            "1    0\n",
            "2    3\n",
            "3    0\n",
            "4    0\n",
            "Name: label, dtype: int64\n",
            "Number of unique user IDs in the test set: 1100\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import entropy\n",
        "import joblib\n",
        "\n",
        "test=np.load(\"../datasets/labeled/first_batch_multi_labels.npz\")\n",
        "\n",
        "X_test=test[\"X\"]\n",
        "y_test=test[\"y\"]\n",
        "\n",
        "# Convert the NumPy array to a Pandas DataFrame\n",
        "df_y_test = pd.DataFrame(y_test, columns=[\"ID\", \"label\"])\n",
        "\n",
        "# Set the \"ID\" column as the index\n",
        "y_test_formatted = df_y_test.set_index(\"ID\")[\"label\"]\n",
        "\n",
        "# Print the first few rows to verify the format\n",
        "print(y_test_formatted.head())\n",
        "\n",
        "y_test = y_test_formatted\n",
        "\n",
        "XX_test = pd.DataFrame(X_test)\n",
        "XX_test.rename(columns={0:\"user\",1:\"item\",2:\"rating\"},inplace=True)\n",
        "\n",
        "num_unique_users = XX_test[\"user\"].nunique()\n",
        "print(f\"Number of unique user IDs in the test set: {num_unique_users}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hprjtSwFBMA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import entropy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "EPS = 1e-6\n",
        "\n",
        "def transform_features(X):\n",
        "    \"\"\"\n",
        "    Transforms the input DataFrame X (assumed to have columns 'user', 'item', 'rating')\n",
        "    into a DataFrame with aggregated user-level features. These include:\n",
        "      - Aggregation of counts and summary statistics.\n",
        "      - Proportional features.\n",
        "      - Gap statistics on item IDs.\n",
        "      - Min, max, median, and variance of movie IDs.\n",
        "      - Product features based on item and rating.\n",
        "      - Rating distribution entropy.\n",
        "      - Movie popularity features.\n",
        "      - Unique movies count and diversity.\n",
        "      - Deviation from population average rating.\n",
        "      - Sequential pattern features (rating differences, etc.).\n",
        "      - Advanced interaction features and review count binning.\n",
        "\n",
        "    Returns a DataFrame with all the computed features.\n",
        "    \"\"\"\n",
        "    df = X.copy()\n",
        "\n",
        "    # --- AGGREGATION: Compute counts and summary statistics ---\n",
        "    user_features = df.groupby(\"user\").agg(\n",
        "        review_count=(\"rating\", \"count\"),\n",
        "        avg_rating=(\"rating\", \"mean\"),\n",
        "        std_rating=(\"rating\", \"std\"),\n",
        "        like_count=(\"rating\", lambda x: (x == 10).sum()),\n",
        "        dislike_count=(\"rating\", lambda x: (x == -10).sum()),\n",
        "        unknown_count=(\"rating\", lambda x: (x == 1).sum()),\n",
        "        neutral_count=(\"rating\", lambda x: (x == 0).sum()),\n",
        "    ).reset_index()\n",
        "\n",
        "\n",
        "    #graph stuff\n",
        "    # # Create the bipartite graph (Users ↔ Movies)\n",
        "    # G = nx.Graph()\n",
        "    # G.add_edges_from(X[['user', 'item']].values)\n",
        "\n",
        "    # # Compute Graph Features\n",
        "    # print(\"Computing graph features...\")\n",
        "\n",
        "    # user_ids = X['user'].unique()\n",
        "\n",
        "    # # Compute core graph features\n",
        "    # degree_centrality = pd.Series(nx.degree_centrality(G), name=\"degree_centrality\")\n",
        "    # pagerank = pd.Series(nx.pagerank(G), name=\"pagerank\")\n",
        "    # betweenness_centrality = pd.Series(nx.betweenness_centrality(G), name=\"betweenness_centrality\")\n",
        "\n",
        "    # # New Feature: Average Neighbor Degree\n",
        "    # avg_neighbor_degree = pd.Series(nx.average_neighbor_degree(G), name=\"avg_neighbor_degree\")\n",
        "\n",
        "    # # New Feature: Ego Network Density\n",
        "    # ego_density = {}\n",
        "    # for user in user_ids:\n",
        "    #     ego_net = nx.ego_graph(G, user)\n",
        "    #     if len(ego_net.nodes) > 1:\n",
        "    #         ego_density[user] = nx.density(ego_net)\n",
        "    #     else:\n",
        "    #         ego_density[user] = 0\n",
        "    # ego_density = pd.Series(ego_density, name=\"ego_density\")\n",
        "\n",
        "    # New Feature: Jaccard Coefficient (Measures similarity between users)\n",
        "    # jaccard_coeff = {}\n",
        "    # for u in user_ids:\n",
        "    #     neighbors = set(G.neighbors(u))\n",
        "    #     scores = [len(neighbors & set(G.neighbors(v))) / len(neighbors | set(G.neighbors(v)))\n",
        "    #               if len(neighbors | set(G.neighbors(v))) > 0 else 0\n",
        "    #               for v in user_ids if v != u]\n",
        "    #     jaccard_coeff[u] = np.mean(scores) if scores else 0\n",
        "    # jaccard_coeff = pd.Series(jaccard_coeff, name=\"jaccard_coefficient\")\n",
        "\n",
        "    # # Combine all graph features into a DataFrame\n",
        "    # graph_features = pd.concat([\n",
        "    #     degree_centrality, pagerank, betweenness_centrality,\n",
        "    #     avg_neighbor_degree, jaccard_coeff\n",
        "    # ], axis=1)\n",
        "\n",
        "    # # Keep only user nodes (filtering out movie nodes)\n",
        "    # graph_features = graph_features.loc[user_ids]\n",
        "    # graph_features = graph_features.reset_index().rename(columns={'index': 'user'})\n",
        "\n",
        "    # user_features = user_features.merge(graph_features, on='user', how='left')\n",
        "\n",
        "    # --- PROPORTIONAL FEATURES ---\n",
        "    user_features[\"like_pct\"] = user_features[\"like_count\"] / (user_features[\"review_count\"] + EPS)\n",
        "    user_features[\"dislike_pct\"] = user_features[\"dislike_count\"] / (user_features[\"review_count\"] + EPS)\n",
        "    user_features[\"unknown_pct\"] = user_features[\"unknown_count\"] / (user_features[\"review_count\"] + EPS)\n",
        "    user_features[\"neutral_pct\"] = user_features[\"neutral_count\"] / (user_features[\"review_count\"] + EPS)\n",
        "\n",
        "    # --- GAP STATISTICS ---\n",
        "    XX_sorted = X.sort_values(by=['user', 'item'])\n",
        "    XX_sorted['item_diff'] = XX_sorted.groupby('user')['item'].diff().fillna(0)\n",
        "    gap_stats = XX_sorted.groupby('user')['item_diff'].agg(['mean', 'std', 'max', 'min'])\n",
        "    gap_stats.columns = ['gap_mean', 'gap_std', 'gap_max', 'gap_min']\n",
        "    user_features = user_features.merge(gap_stats, on='user', how='left')\n",
        "\n",
        "    # --- MIN/MAX/MEDIAN/VARIANCE OF MOVIE IDs ---\n",
        "    min_max_df = X.groupby(\"user\")[\"item\"].agg(min_movie=\"min\", max_movie=\"max\", median_movie=\"median\", variance_movie=\"var\").reset_index()\n",
        "    user_features = user_features.merge(min_max_df, on=\"user\", how=\"left\")\n",
        "\n",
        "    # --- PRODUCT FEATURES ---\n",
        "    X = X.copy()  # work on a copy to avoid modifying original data\n",
        "    X[\"item_rating\"] = X[\"item\"] * X[\"rating\"]\n",
        "    sum_rating = X.groupby(\"user\")[\"rating\"].sum().reset_index(name=\"sum_rating\")\n",
        "    sum_product = X.groupby(\"user\")[\"item_rating\"].sum().reset_index(name=\"sum_item_rating\")\n",
        "    user_features = user_features.merge(sum_product, on=\"user\", how=\"left\")\n",
        "    user_features = user_features.merge(sum_rating, on=\"user\", how=\"left\")\n",
        "\n",
        "    user_features[\"average_product\"] = user_features[\"sum_item_rating\"] / user_features[\"review_count\"]\n",
        "    user_features[\"product_above_zero\"] = (user_features[\"sum_item_rating\"] > 0).astype(int)\n",
        "    user_features[\"sum_above_zero\"] = (user_features[\"sum_rating\"] > 0).astype(int)\n",
        "    user_features[\"avg_product_vs_avg_rating\"] = user_features[\"average_product\"] / (user_features[\"avg_rating\"] + EPS)\n",
        "\n",
        "    # --- RATING DISTRIBUTION ENTROPY ---\n",
        "    def calc_entropy(row):\n",
        "        probs = [row[\"like_pct\"], row[\"dislike_pct\"], row[\"unknown_pct\"], row[\"neutral_pct\"]]\n",
        "        probs = [p for p in probs if p > 0]\n",
        "        return entropy(probs) if probs else 0\n",
        "    user_features[\"rating_entropy\"] = user_features.apply(calc_entropy, axis=1)\n",
        "\n",
        "    # --- MOVIE POPULARITY FEATURES ---\n",
        "    movie_popularity = X.groupby(\"item\").size().reset_index(name=\"movie_popularity\")\n",
        "    X_with_pop = X.merge(movie_popularity, on=\"item\")\n",
        "    pop_features = X_with_pop.groupby(\"user\").agg(\n",
        "        avg_movie_popularity=(\"movie_popularity\", \"mean\"),\n",
        "        std_movie_popularity=(\"movie_popularity\", \"std\"),\n",
        "        min_movie_popularity=(\"movie_popularity\", \"min\"),\n",
        "        max_movie_popularity=(\"movie_popularity\", \"max\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    # --- UNIQUE MOVIES AND DIVERSITY ---\n",
        "    unique_items = X.groupby(\"user\")[\"item\"].nunique().reset_index()\n",
        "    unique_items.columns = [\"user\", \"unique_movies\"]\n",
        "\n",
        "    # --- DEVIATION FROM POPULATION FEATURES ---\n",
        "    movie_avg_rating = X.groupby(\"item\")[\"rating\"].mean().reset_index(name=\"movie_avg_rating\")\n",
        "    X_with_avg = X.merge(movie_avg_rating, on=\"item\")\n",
        "    X_with_avg[\"rating_deviation\"] = X_with_avg[\"rating\"] - X_with_avg[\"movie_avg_rating\"]\n",
        "    X_with_avg[\"abs_rating_deviation\"] = np.abs(X_with_avg[\"rating_deviation\"])\n",
        "    deviation_features = X_with_avg.groupby(\"user\").agg(\n",
        "        mean_deviation=(\"rating_deviation\", \"mean\"),\n",
        "        std_deviation=(\"rating_deviation\", \"std\"),\n",
        "        mean_abs_deviation=(\"abs_rating_deviation\", \"mean\"),\n",
        "        max_abs_deviation=(\"abs_rating_deviation\", \"max\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    # --- SEQUENTIAL PATTERN FEATURES ---\n",
        "    X_sorted = X.sort_values([\"user\", \"item\"])\n",
        "    X_sorted[\"next_rating\"] = X_sorted.groupby(\"user\")[\"rating\"].shift(-1)\n",
        "    X_sorted[\"rating_diff\"] = X_sorted[\"next_rating\"] - X_sorted[\"rating\"]\n",
        "    X_sorted[\"abs_rating_diff\"] = np.abs(X_sorted[\"rating_diff\"])\n",
        "    X_sorted = X_sorted.dropna(subset=[\"rating_diff\"])\n",
        "    sequence_features = X_sorted.groupby(\"user\").agg(\n",
        "        mean_rating_diff=(\"rating_diff\", \"mean\"),\n",
        "        std_rating_diff=(\"rating_diff\", \"std\"),\n",
        "        mean_abs_rating_diff=(\"abs_rating_diff\", \"mean\"),\n",
        "        max_abs_rating_diff=(\"abs_rating_diff\", \"max\"),\n",
        "        rating_changes_count=(\"rating_diff\", lambda x: (x != 0).sum()),\n",
        "    ).reset_index()\n",
        "    sequence_features[\"rating_changes_pct\"] = sequence_features[\"rating_changes_count\"] / (\n",
        "        user_features.set_index(\"user\")[\"review_count\"] - 1 + EPS\n",
        "    ).reindex(sequence_features[\"user\"]).values\n",
        "\n",
        "    # --- COMBINE ALL FEATURES ---\n",
        "    all_features = user_features.merge(pop_features, on=\"user\", how=\"left\")\n",
        "    all_features = all_features.merge(unique_items, on=\"user\", how=\"left\")\n",
        "    all_features = all_features.merge(deviation_features, on=\"user\", how=\"left\")\n",
        "    all_features = all_features.merge(sequence_features, on=\"user\", how=\"left\")\n",
        "\n",
        "    all_features[\"diversity_ratio\"] = all_features[\"unique_movies\"] / (all_features[\"review_count\"] + EPS)\n",
        "    all_features = all_features.fillna(0)\n",
        "\n",
        "    # --- UNSUPERVISED ANOMALY DETECTION FEATURES ---\n",
        "    feature_cols = [col for col in all_features.columns\n",
        "                    if col not in [\"user\", \"label\", \"is_anomalous\"]\n",
        "                    and all_features[col].dtype in [np.float64, np.int64]]\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = scaler.fit_transform(all_features[feature_cols])\n",
        "\n",
        "    # --- ADVANCED INTERACTION FEATURES ---\n",
        "    all_features[\"like_dislike_ratio\"] = all_features[\"like_count\"] / (all_features[\"dislike_count\"] + EPS)\n",
        "    all_features[\"rating_range\"] = all_features[\"max_abs_rating_diff\"]\n",
        "    all_features[\"popularity_vs_deviation\"] = all_features[\"avg_movie_popularity\"] * all_features[\"mean_abs_deviation\"]\n",
        "    all_features[\"entropy_by_count\"] = all_features[\"rating_entropy\"] * np.log1p(all_features[\"review_count\"])\n",
        "\n",
        "    # --- BINNING FEATURES ---\n",
        "    all_features[\"review_count_bin\"] = pd.qcut(all_features[\"review_count\"], q=5, labels=False, duplicates=\"drop\")\n",
        "\n",
        "    return pd.DataFrame(all_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRt-PQQ4Biv6",
        "outputId": "6a93053f-949f-462c-ff0e-5bce3e2fa995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_features before selecting features (1100, 51)\n",
            "test_features after selecting features (1100, 41)\n",
            "Index(['review_count', 'avg_rating', 'std_rating', 'like_count',\n",
            "       'dislike_count', 'neutral_count', 'dislike_pct', 'unknown_pct',\n",
            "       'neutral_pct', 'gap_mean', 'gap_max', 'gap_min', 'rating_entropy',\n",
            "       'svd_1', 'svd_2', 'svd_3', 'avg_movie_popularity',\n",
            "       'std_movie_popularity', 'min_movie_popularity', 'max_movie_popularity',\n",
            "       'std_deviation', 'mean_abs_deviation', 'max_abs_deviation',\n",
            "       'mean_rating_diff', 'std_rating_diff', 'max_abs_rating_diff',\n",
            "       'rating_changes_pct', 'diversity_ratio', 'like_dislike_ratio',\n",
            "       'popularity_vs_deviation', 'entropy_by_count', 'review_count_bin',\n",
            "       'min_movie', 'max_movie', 'median_movie', 'variance_movie',\n",
            "       'sum_item_rating', 'average_product', 'product_above_zero',\n",
            "       'sum_above_zero', 'avg_product_vs_avg_rating'],\n",
            "      dtype='object')\n",
            "   review_count  avg_rating  std_rating  like_count  dislike_count  \\\n",
            "0           168    5.946429    5.253181         100              5   \n",
            "1           208    3.158654    5.890205          76             16   \n",
            "2           195    1.025641    7.750913          66             52   \n",
            "3            41    1.073171    6.455193          10              7   \n",
            "4             6    6.833333    4.915960           4              0   \n",
            "\n",
            "   neutral_count  dislike_pct  unknown_pct  neutral_pct   gap_mean  ...  \\\n",
            "0             14     0.029762     0.291667     0.083333   4.696429  ...   \n",
            "1             59     0.076923     0.274038     0.283654   3.673077  ...   \n",
            "2             17     0.266667     0.307692     0.087179   4.492308  ...   \n",
            "3             10     0.170732     0.341463     0.243902  10.634146  ...   \n",
            "4              1     0.000000     0.166667     0.166667  89.666667  ...   \n",
            "\n",
            "   review_count_bin  min_movie  max_movie  median_movie  variance_movie  \\\n",
            "0                 3          9        798         319.5    53415.140576   \n",
            "1                 3          0        764         341.0    40799.608115   \n",
            "2                 3          3        879         400.0    67011.127412   \n",
            "3                 0         15        451         247.0    17024.002439   \n",
            "4                 0          0        538         386.0    42841.766667   \n",
            "\n",
            "   sum_item_rating  average_product  product_above_zero  sum_above_zero  \\\n",
            "0           348353      2073.529762                   1               1   \n",
            "1           234468      1127.250000                   1               1   \n",
            "2            63898       327.682051                   1               1   \n",
            "3             3730        90.975610                   1               1   \n",
            "4            11532      1922.000000                   1               1   \n",
            "\n",
            "   avg_product_vs_avg_rating  \n",
            "0                 348.701643  \n",
            "1                 356.876599  \n",
            "2                 319.489688  \n",
            "3                  84.772648  \n",
            "4                 281.268252  \n",
            "\n",
            "[5 rows x 41 columns]\n",
            "[[0.79545455 0.03409091 0.06818182 0.         0.04545455 0.05681818]\n",
            " [0.71590909 0.10227273 0.09090909 0.         0.06818182 0.02272727]\n",
            " [0.28409091 0.01136364 0.01136364 0.64772727 0.03409091 0.01136364]\n",
            " ...\n",
            " [0.875      0.02272727 0.02272727 0.02272727 0.04545455 0.01136364]\n",
            " [0.89772727 0.04545455 0.04545455 0.         0.01136364 0.        ]\n",
            " [0.79545455 0.01136364 0.05681818 0.         0.05681818 0.07954545]]\n"
          ]
        }
      ],
      "source": [
        "# Feature engineering\n",
        "test_features = transform_features(XX_test)\n",
        "test_features.sort_values(by=\"user\", inplace=True) #Sort by user\n",
        "\n",
        "# Select only important features\n",
        "model_features = joblib.load(\"model_features.pkl\")\n",
        "\n",
        "print(f\"test_features before selecting features {test_features.shape}\")\n",
        "\n",
        "# If feature does not exist, populate with 0s\n",
        "for feat in model_features:\n",
        "    if feat not in test_features.columns:\n",
        "        test_features[feat] = 0\n",
        "test_features = test_features[model_features]\n",
        "\n",
        "print(f\"test_features after selecting features {test_features.shape}\")\n",
        "\n",
        "\n",
        "print(test_features.columns)\n",
        "print(test_features.head())\n",
        "\n",
        "# Load scaler and polynomial transformer, and transform the test features\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "poly = joblib.load(\"poly.pkl\")\n",
        "test_features_scaled = scaler.transform(test_features)\n",
        "test_features_poly = poly.transform(test_features_scaled)\n",
        "\n",
        "# Load the trained model and predict probabilities (shape: #test_users x 6)\n",
        "rf_model = joblib.load(\"model.pkl\")\n",
        "# probabilities = rf_model.predict_proba(test_features_poly)\n",
        "y_pred_proba_rf = rf_model.predict_proba(test_features_poly)\n",
        "print(y_pred_proba_rf)\n",
        "\n",
        "\n",
        "# import joblib\n",
        "\n",
        "# # Save the predictions as an .npz file\n",
        "# np.savez(\"predictions.npz\", probabilities=probabilities)\n",
        "# print(f\"prediction shape {probabilities.shape}\")\n",
        "\n",
        "# # View predictions.npz\n",
        "# test_results=np.load(\"predictions.npz\")\n",
        "# test_results_df = pd.DataFrame(test_results[\"probabilities\"])\n",
        "# print(test_results_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf__Rb7r0qIh",
        "outputId": "a6ea1c66-23d3-4102-93d3-fb91d816adec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForest AUC Scores per Class:\n",
            "ID\n",
            "0       0\n",
            "1       0\n",
            "2       3\n",
            "3       0\n",
            "4       0\n",
            "       ..\n",
            "1095    0\n",
            "1096    3\n",
            "1097    0\n",
            "1098    0\n",
            "1099    0\n",
            "Name: label, Length: 1100, dtype: int64\n",
            "  Class 0: AUC = 0.784\n",
            "  Class 1: AUC = 0.669\n",
            "  Class 2: AUC = 0.709\n",
            "  Class 3: AUC = 0.997\n",
            "  Class 4: AUC = 0.539\n",
            "  Class 5: AUC = 0.605\n",
            "\n",
            "🏆 Final Evaluation Metric: 0.744\n"
          ]
        }
      ],
      "source": [
        "print(\"RandomForest AUC Scores per Class:\")\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print(y_test)\n",
        "auc_per_class_rf = {}\n",
        "for idx, cls in enumerate(rf_model.classes_):\n",
        "    binary_true = (y_test == cls).astype(int)\n",
        "    try:\n",
        "        auc = roc_auc_score(binary_true, y_pred_proba_rf[:, idx])\n",
        "        auc_per_class_rf[cls] = auc\n",
        "        print(f\"  Class {cls}: AUC = {auc:.3f}\")\n",
        "    except Exception as e:\n",
        "        auc_per_class_rf[cls] = None\n",
        "        print(e)\n",
        "        print(f\"  Class {cls}: AUC could not be computed\")\n",
        "\n",
        "k = 5\n",
        "AUC_0 = auc_per_class_rf[0]\n",
        "anomaly_aucs = [auc_per_class_rf[i] for i in range(1, k+1) if i in auc_per_class_rf]\n",
        "\n",
        "final_metric = (0.5 * AUC_0) + (0.5 / k) * sum(anomaly_aucs)\n",
        "\n",
        "print(f\"\\n🏆 Final Evaluation Metric: {final_metric:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
